{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Lloyd's algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lloyd(data, k : int):\n",
    "    \"\"\"\n",
    "    Divide two numbers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : nxd matrix of n samples with d features\n",
    "    k : number of clusters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary associating each centroid to the indices of data points assigned to it\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(data, pd.DataFrame), \"data must be a pandas DataFrame\"\n",
    "    assert isinstance(k, int), \"k must be an int\"\n",
    "    assert k <= len(data), \"k must be at most the number of samples\"\n",
    "\n",
    "    # sample k row indexes and make them centroids\n",
    "    centroid_ids = np.random.choice(len(data), k, replace=False)\n",
    "    centroids = data.loc[centroid_ids, :]\n",
    "    print('init centroids ids:', centroid_ids)\n",
    "    print('init centroids:\\n', centroids)\n",
    "\n",
    "    closest_points_ids = None\n",
    "    while True:\n",
    "        new_centroids, closest_points_ids = move_centroids(data, centroids)\n",
    "\n",
    "        # break at convergence\n",
    "        if new_centroids.equals(centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return centroids, closest_points_ids\n",
    "\n",
    "\n",
    "def move_centroids(data, centroids):\n",
    "    # new_centroids is analogous to centroids\n",
    "    new_centroids = pd.DataFrame(pd.NA, index=centroids.index, columns=centroids.columns)\n",
    "    \n",
    "    # distances is a (n x k) DataFrame\n",
    "    distances = pd.DataFrame(index=data.index, columns=centroids.index, dtype=float)\n",
    "\n",
    "    # iterate on all datapoints\n",
    "    for point_id in distances.index:\n",
    "        point = data.loc[point_id, :]\n",
    "\n",
    "        # calculate distance of point from centroids\n",
    "        for centroid_id in centroids.index:\n",
    "            centroid = centroids.loc[centroid_id, :]\n",
    "            distances.loc[point_id, centroid_id] = distance(point, centroid )\n",
    "\n",
    "    # assign points to closest centroid\n",
    "    min_centroids = distances.idxmin(axis='columns')\n",
    "    \n",
    "    closest_points_ids = np.array(se)\n",
    "    for centroid_id in distances.columns:\n",
    "        ids = min_centroids[min_centroids == centroid_id].index\n",
    "        closest_points_ids[centroid_id] = ids\n",
    "        \n",
    "        closest_points = data.loc[ids.array, :]\n",
    "        new_centroids.loc[centroid_id, :] = closest_points.mean()\n",
    "    \n",
    "    return new_centroids, closest_points_ids\n",
    "\n",
    "def distance(x1, x2):\n",
    "    return np.square(np.linalg.norm(x1-x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lloyd:\n",
    "    \"\"\"\n",
    "    Perform k-means clustering using standard Lloyd's algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.centroids = None\n",
    "        self.y_pred = None\n",
    "    \n",
    "    \n",
    "    def fit(self, data : pd.DataFrame, k : int, seed : int = None):\n",
    "        \"\"\"\n",
    "        Fit the model to the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pd.DataFrame\n",
    "            nxd DataFrame of n samples with d features\n",
    "        k : int\n",
    "            Number of clusters\n",
    "        seed : int\n",
    "            Seed for random generator\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Mapping from each centroid to the indices of data points assigned to it\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(data, pd.DataFrame), \"data must be a pandas DataFrame\"\n",
    "        assert isinstance(k, int), \"k must be an int\"\n",
    "        assert k <= len(data), \"k must be at most the number of samples\"\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # sample k row indexes and make them centroids\n",
    "        centroid_ids = np.random.choice(len(data), k, replace=False)\n",
    "        centroids = data.loc[centroid_ids, :]\n",
    "        print('init centroids ids:', centroid_ids)\n",
    "        print('init centroids:\\n', centroids)\n",
    "\n",
    "        closest_points_ids = None\n",
    "        while True:\n",
    "            new_centroids, closest_points_ids = self._move_centroids(data, centroids)\n",
    "\n",
    "            # break at convergence\n",
    "            if new_centroids.equals(centroids):\n",
    "                break\n",
    "            centroids = new_centroids\n",
    "\n",
    "        self.centroids = centroids\n",
    "        self.y_pred = closest_points_ids\n",
    "\n",
    "        return centroids, closest_points_ids\n",
    "\n",
    "    def _move_centroids(self, data, centroids):\n",
    "        # new_centroids is analogous to centroids\n",
    "        new_centroids = pd.DataFrame(pd.NA, index=centroids.index, columns=centroids.columns)\n",
    "\n",
    "        # distances is a (n x k) DataFrame\n",
    "        distances = pd.DataFrame(index=data.index, columns=centroids.index, dtype=float)\n",
    "\n",
    "        # iterate on all datapoints\n",
    "        for point_id in distances.index:\n",
    "            point = data.loc[point_id, :]\n",
    "\n",
    "            # calculate distance of point from centroids\n",
    "            for centroid_id in centroids.index:\n",
    "                centroid = centroids.loc[centroid_id, :]\n",
    "                distances.loc[point_id, centroid_id] = self._distance(point, centroid )\n",
    "\n",
    "        # assign points to closest centroid\n",
    "        min_centroids = distances.idxmin(axis='columns')\n",
    "        closest_points_ids = np.zeros(self.data.shape[0], dtype=int)\n",
    "\n",
    "        for centroid_id in distances.columns:\n",
    "            ids = min_centroids[min_centroids == centroid_id].index\n",
    "            \n",
    "            closest_points_ids[ids] = centroid_id\n",
    "            \n",
    "            closest_points = data.loc[ids.array, :]\n",
    "            new_centroids.loc[centroid_id, :] = closest_points.mean()\n",
    "\n",
    "        return new_centroids, closest_points_ids\n",
    "\n",
    "    def _distance(self, x1, x2):\n",
    "        return np.square(np.linalg.norm(x1-x2))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true : np.ndarray, y_pred : np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of the clustering.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        True labels of the samples\n",
    "    y_pred : np.ndarray\n",
    "        Predicted labels of the samples\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Accuracy of the clustering through Hungarian algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(y_true, np.ndarray), \"y_true must be a numpy array\"\n",
    "    assert isinstance(y_pred, np.ndarray), \"y_pred must be a numpy array\"\n",
    "\n",
    "    # create C matrix\n",
    "    n_classes = max(max(y_true), max(y_pred)) + 1\n",
    "    C = np.zeros((n_classes, n_classes), dtype=int)\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        C[true_label, pred_label] += 1\n",
    "    \n",
    "    # Solve assignment problem\n",
    "    row_ind, col_ind = linear_sum_assignment(-C)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    matched = C[row_ind, col_ind].sum()\n",
    "    accuracy = matched / len(y_true)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init centroids ids: [2 0]\n",
      "init centroids:\n",
      "     0   1\n",
      "2  20  20\n",
      "0   0   0\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame([[0,0],[1,1],[20,20],[21,21]])\n",
    "\n",
    "k_means = Lloyd()\n",
    "centroids, y_pred = k_means.fit(data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_pred, np.array([0,0,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init centroids ids: [521 737 740 660 411 678 626 513 859 136]\n",
      "init centroids:\n",
      "      Feature1  Feature2  Feature3  Feature4   Feature5  Feature6  Feature7  \\\n",
      "521 -9.558033 -0.955288  5.004327  2.323583  10.305739 -3.608574 -0.637341   \n",
      "737 -8.557540 -3.506088  4.063249  4.344668  11.528762 -4.539703  0.291597   \n",
      "740 -2.028188  9.238170  3.849404  2.444638  -4.997603 -5.534690 -7.245141   \n",
      "660  2.808011  5.519617 -4.609808 -3.618455  -0.860509 -4.555741  8.484914   \n",
      "411 -1.945288  5.430327 -7.428246  8.323527   0.305620  8.121266 -2.147271   \n",
      "678  3.896995  2.590766  7.899939 -4.026307   5.188462  0.196091  8.479080   \n",
      "626 -3.695540 -0.143381 -5.393527  0.590220   9.349823  1.218455 -2.668507   \n",
      "513  1.775394  4.277720  2.084934 -9.060278  -6.078125  3.394702 -5.391960   \n",
      "859  6.031470  0.883097  7.280661 -4.723444  -0.865785  5.271398  0.122286   \n",
      "136  0.435323 -4.382507  7.958847 -9.596048   8.778427  5.719405  0.908802   \n",
      "\n",
      "     Feature8   Feature9  Feature10  \n",
      "521 -2.243475  -2.854701   5.644246  \n",
      "737 -2.456914  -1.998832   7.505817  \n",
      "740  6.812307   1.032695   4.035665  \n",
      "660 -1.660433   6.500868   0.996324  \n",
      "411  3.790708   1.528636   3.961170  \n",
      "678 -0.280738   0.904561   1.213444  \n",
      "626  5.941608  -7.174105   7.459652  \n",
      "513 -8.337393  -2.058602   0.759804  \n",
      "859  1.959100 -10.222697  -2.329587  \n",
      "136 -9.369098  -0.165994  -1.856130  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.86)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_test_dataframe(n_clusters=3, n_samples=150, n_features=2, seed=None):\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Calculate points per cluster\n",
    "    points_per_cluster = n_samples // n_clusters\n",
    "    remainder = n_samples % n_clusters  # To handle uneven division\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for cluster_id in range(n_clusters):\n",
    "        # Generate random cluster center\n",
    "        center = np.random.uniform(-10, 10, size=n_features)\n",
    "        \n",
    "        # Generate points for this cluster\n",
    "        n_points = points_per_cluster + (1 if cluster_id < remainder else 0)\n",
    "        points = np.random.normal(loc=center, scale=1.0, size=(n_points, n_features))\n",
    "        \n",
    "        data.append(points)\n",
    "        labels.extend([cluster_id] * n_points)\n",
    "    \n",
    "    # Combine all clusters into one dataset\n",
    "    data = np.vstack(data)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    indices = np.arange(len(labels))\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels = labels[indices]\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    column_names = [f\"Feature{i+1}\" for i in range(n_features)]\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "    df[\"Label\"] = labels\n",
    "    \n",
    "    return df\n",
    "\n",
    "test_df = generate_test_dataframe(10, 1000, 10, seed=42)\n",
    "\n",
    "k_means = Lloyd()\n",
    "k_means.fit(test_df.iloc[:, :-1], 10, seed=42)\n",
    "accuracy(np.array(test_df[\"Label\"]), k_means.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "data = mnist.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
