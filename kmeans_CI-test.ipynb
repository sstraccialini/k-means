{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4905f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Literal\n",
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09dbd64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    \"\"\"\n",
    "    Perform KMeans clustering on a dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 algorithm : Literal['lloyd', 'extended-hartigan', 'safe-hartigan', 'hartigan', 'binary-hartigan', 'mixed-hartigan', 'mixed-extended-hartigan'] = 'lloyd',\n",
    "                 init : Literal['random', 'random-data', 'k-means++', 'maximin', 'greedy-k-means++'] = 'random',\n",
    "                 seed : Union[int, None] = None):\n",
    "        \"\"\"\n",
    "        Initialize the KMeans object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        algorithm : {'lloyd', 'extended-hartigan', 'safe-hartigan', 'hartigan', 'binary-hartigan', 'mixed-hartigan', 'mixed-extended-hartigan'}\n",
    "            Algorithm to use. Either 'lloyd' or 'extended-hartigan' or 'safe-hartigan' or 'hartigan' or 'binary-hartigan'\n",
    "\n",
    "        init : {'random', 'random-data', 'k-means++', 'maximin', 'greedy-k-means++'}\n",
    "            Initialization method. Either 'random' or 'random-data' or 'k-means++' or 'maximin' or 'greedy-k-means++'\n",
    "\n",
    "        seed : int\n",
    "            Seed for random generator\n",
    "        \"\"\"\n",
    "\n",
    "        assert algorithm in ['lloyd', 'extended-hartigan', 'safe-hartigan', 'hartigan', 'binary-hartigan', 'mixed-hartigan', 'mixed-extended-hartigan'], \"algorithm must be either 'lloyd', 'extended-hartigan', 'safe-hartigan', 'hartigan', 'binary-hartigan', 'mixed-hartigan' or 'mixed-extended-hartigan'\"\n",
    "        assert init in ['random', 'random-data', 'k-means++', 'maximin', 'greedy-k-means++'], \"init must be either 'random', 'random-data', 'k-means++', 'maximin', or 'greedy-k-means++'\"\n",
    "        assert seed is None or isinstance(seed, int), \"seed must be an int or None\"\n",
    "\n",
    "        self.algorithm = algorithm\n",
    "        self.init = init\n",
    "        self.seed = seed\n",
    "\n",
    "        self.data = None\n",
    "        self.k = None\n",
    "        self.n_samples = None\n",
    "        self.centroids = None\n",
    "        self.y_pred = None\n",
    "\n",
    "        self.safe_iterations = 0\n",
    "        self.unsafe_iterations = 0\n",
    "        self.lloyd_iterations = 0\n",
    "        self.hartigan_iterations = 0\n",
    "        self.binary_iterations = 0\n",
    "\n",
    "        self.norm_calculations = 0\n",
    "        self.init_norm_calculations = 0\n",
    "\n",
    "\n",
    "    def fit(self, data : np.ndarray, k : int, debug : int = 0):\n",
    "        \"\"\"\n",
    "        Fit the model to the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.ndarray\n",
    "            nxd DataFrame of n samples with d features\n",
    "        k : int\n",
    "            Number of clusters\n",
    "        debug : int\n",
    "            Debug level (0: no debug, 1: some debug, 2: all debug)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array of shape (k, d) with cluster centroids\n",
    "        np.ndarray\n",
    "            Array of length n with cluster assignments for each sample\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(data, np.ndarray), \"data must be a numpy array\"\n",
    "        assert len(data.shape) == 2, \"data must be a 2D array\"\n",
    "        assert isinstance(k, int), \"k must be an int\"\n",
    "        assert 0 < k <= len(data), \"k must be at least 0 and at most the number of samples\"\n",
    "        assert isinstance(debug, int) or debug, \"debug must be an int\"\n",
    "\n",
    "        self.data = data\n",
    "        self.k = k\n",
    "        self.n_samples = data.shape[0]\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        # initialize centroids\n",
    "        self._init_centroids(debug)\n",
    "        debug and print('initial centroids:\\n', self.centroids)\n",
    "        debug and print('initial y_pred:', self.y_pred)\n",
    "\n",
    "        if self.algorithm == 'lloyd':\n",
    "            self._lloyd(debug)\n",
    "        elif self.algorithm == 'extended-hartigan':\n",
    "            self._extended_hartigan(always_safe=False, debug=debug)\n",
    "        elif self.algorithm == 'safe-hartigan':\n",
    "            self._extended_hartigan(always_safe=True, binary_hartigan=False, debug=debug)\n",
    "        elif self.algorithm == 'binary-hartigan':\n",
    "            self._extended_hartigan(always_safe=False, binary_hartigan=True, debug=debug)\n",
    "        elif self.algorithm == 'hartigan':\n",
    "            self._hartigan(debug)\n",
    "        elif self.algorithm == 'mixed-hartigan':\n",
    "            self._lloyd(debug)\n",
    "            self._hartigan(debug)\n",
    "        elif self.algorithm == 'mixed-extended-hartigan':\n",
    "            self._lloyd(debug)\n",
    "            self._extended_hartigan(always_safe=False, binary_hartigan=False, debug=debug)\n",
    "        \n",
    "        debug and print('final centroids:\\n', self.centroids)\n",
    "        debug and print('final y_pred:', self.y_pred)\n",
    "\n",
    "\n",
    "    def _init_centroids(self, debug=0):\n",
    "        \"\"\"\n",
    "        Initialize the centroids.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.init == 'random':\n",
    "\n",
    "            # choose k random data points as initial centroids\n",
    "            idx = np.random.choice(self.n_samples, self.k, replace=False)\n",
    "            self.centroids = self.data[idx]\n",
    "            self.y_pred = self._assign_clusters(debug=debug>1, init_call=True)\n",
    "\n",
    "        elif self.init == 'random-data':\n",
    "            return NotImplemented\n",
    "            \n",
    "            # V1\n",
    "            # assign each data point to a random cluster\n",
    "            clusters = np.random.choice(self.k, self.n_samples)\n",
    "\n",
    "            # check that at least one point is assigned to each cluster\n",
    "            while len(set(clusters)) < self.k:\n",
    "                clusters = np.random.choice(self.k, self.n_samples)\n",
    "            self.y_pred = clusters\n",
    "            self.centroids = self._move_centroids(None, debug > 1)\n",
    "\n",
    "        elif self.init == 'k-means++':\n",
    "            \n",
    "            # choose first centroid randomly\n",
    "            centroids = np.zeros((self.k, self.data.shape[1]))\n",
    "            centroids[0] = self.data[np.random.choice(self.n_samples, 1, replace=False)[0]]\n",
    "            debug and print('centroids:\\n', centroids)\n",
    "\n",
    "            # iterate over remaining k-1 centroids\n",
    "            for i in range(1, self.k):\n",
    "                debug and print('iteration', i)\n",
    "\n",
    "                # calculate squared distance of each point to closest centroid\n",
    "                dist = np.min(np.linalg.norm(self.data[:, np.newaxis] - centroids[:i], axis=2) ** 2, axis=1)\n",
    "                self.init_norm_calculations += (self.n_samples * i) / (self.k*self.n_samples)\n",
    "\n",
    "                # probabilities are given by the normalized distance squared\n",
    "                probs = dist / dist.sum()\n",
    "                debug and print('probs:', probs)\n",
    "\n",
    "                # choose next centroid randomly based on cumulated probabilities\n",
    "                j = np.random.choice(self.n_samples, p=probs)\n",
    "\n",
    "                centroids[i] = self.data[j]\n",
    "                debug and print('centroids:\\n', centroids)\n",
    "\n",
    "            self.centroids = centroids\n",
    "            self.y_pred = self._assign_clusters(debug=debug>1, init_call=True)\n",
    "\n",
    "        elif self.init == 'maximin':\n",
    "\n",
    "            # choose first centroid randomly\n",
    "            centroids = np.zeros((self.k, self.data.shape[1]))\n",
    "            centroids[0] = self.data[np.random.choice(self.n_samples, 1, replace=False)[0]]\n",
    "            debug and print('centroids:\\n', centroids)\n",
    "\n",
    "            # iterate over remaining k-1 centroids\n",
    "            for i in range(1, self.k):\n",
    "                debug and print('iteration', i)\n",
    "\n",
    "                # calculate squared distance of each point to closest centroid\n",
    "                dist = np.min(np.linalg.norm(self.data[:, np.newaxis] - centroids[:i], axis=2) ** 2, axis=1)\n",
    "                self.init_norm_calculations += (self.n_samples * i) / (self.k*self.n_samples)\n",
    "\n",
    "                # choose next centroid as the point with the maximum distance to the closest centroid\n",
    "                centroids[i] = self.data[np.argmax(dist)]\n",
    "                debug and print('centroids:\\n', centroids)\n",
    "\n",
    "            self.centroids = centroids\n",
    "            self.y_pred = self._assign_clusters(debug=debug>1, init_call=True)\n",
    "\n",
    "        elif self.init == 'greedy-k-means++':\n",
    "\n",
    "            # TODO: this might be adapted\n",
    "            trials = 2 + int(np.log(self.k)) \n",
    "\n",
    "            # choose first centroid randomly\n",
    "            centroids = np.zeros((self.k, self.data.shape[1]))\n",
    "            f = np.random.choice(self.n_samples)\n",
    "            centroids[0] = self.data[f]\n",
    "            debug and print('centroids:\\n', centroids)\n",
    "\n",
    "            # iterate over remaining k-1 centroids\n",
    "            for i in range(1, self.k):\n",
    "                debug and print('iteration', i)\n",
    "\n",
    "                # calculate squared distance of each point to closest centroid\n",
    "                dist = np.min(np.linalg.norm(self.data[:, np.newaxis] - centroids[:i], axis=2) ** 2, axis=1)\n",
    "                self.init_norm_calculations += (self.n_samples * i) / (self.k*self.n_samples)\n",
    "\n",
    "                # probabilities are given by the normalized distance squared\n",
    "                probs = dist / dist.sum()\n",
    "                debug and print('probs:', probs)\n",
    "\n",
    "                # choose next centroid randomly based on cumulated probabilities\n",
    "                candidate_j = np.random.choice(self.n_samples, size=trials, p=probs)\n",
    "                debug and print('candidate_j:', candidate_j)\n",
    "\n",
    "                # calculate the cost of each trial\n",
    "                costs = np.zeros(trials)\n",
    "                for t in range(trials):\n",
    "                    centroids[i] = self.data[candidate_j[t]]\n",
    "                    costs[t] = self._tot_cluster_cost(centroids, self._assign_clusters(specific_centroids=centroids, init_call=True), debug=debug>1, init_call=True)\n",
    "                debug and print('costs:', costs)\n",
    "\n",
    "                # choose the trial with the lowest cost\n",
    "                j = candidate_j[np.argmin(costs)]\n",
    "\n",
    "                centroids[i] = self.data[j]\n",
    "                debug and print('centroids:\\n', centroids)\n",
    "\n",
    "            self.centroids = centroids\n",
    "            self.y_pred = self._assign_clusters(debug=debug>1, init_call=True)\n",
    "\n",
    "\n",
    "    def _lloyd(self, debug=0):\n",
    "        \"\"\"\n",
    "        Lloyd's algorithm for k-means clustering.\n",
    "        \"\"\"\n",
    "\n",
    "        debug and print('\\nRunning Lloyd\\'s algorithm...')\n",
    "        old_y_pred = np.zeros(self.n_samples, dtype=int)\n",
    "\n",
    "        while True:\n",
    "            \n",
    "            debug and print('New iteration')\n",
    "            self.lloyd_iterations += 1\n",
    "\n",
    "            np.copyto(old_y_pred, self.y_pred)\n",
    "\n",
    "            # move centroids to the mean of their cluster\n",
    "            new_centroids = self._move_centroids(None, debug > 1)\n",
    "            self.centroids = new_centroids\n",
    "\n",
    "            # assign each data point to the closest centroid\n",
    "            self.y_pred = self._assign_clusters(debug=debug>1)\n",
    "            debug and print('y_pred:', self.y_pred)\n",
    "\n",
    "            # check for convergence\n",
    "            if np.array_equal(old_y_pred, self.y_pred):\n",
    "                break\n",
    "\n",
    "\n",
    "    def _extended_hartigan(self, always_safe=False, binary_hartigan=False, debug=0):\n",
    "        \"\"\"\n",
    "        Extended Hartigan algorithm for k-means clustering (unsafe+safe, always safe or binary mode).\n",
    "        \"\"\"\n",
    "\n",
    "        debug and print('\\nRunning Extended Hartigan algorithm...')\n",
    "\n",
    "        # move centroids to the mean of their cluster\n",
    "        self.centroids = self._move_centroids(None, debug > 1)\n",
    "\n",
    "        while True:\n",
    "            self.unsafe_iterations += 1\n",
    "\n",
    "            # start with unsafe mode    \n",
    "            safe_mode = False\n",
    "\n",
    "            # create an empty dictionary of new candidates\n",
    "            candidates = {}\n",
    "            \n",
    "            # calculate sizes of clusters and cost of current assignment\n",
    "            cluster_sizes = np.bincount(self.y_pred, minlength=self.k)\n",
    "\n",
    "            for datapoint_id in range(self.n_samples):\n",
    "                debug and print('\\ndatapoint_id:', datapoint_id)\n",
    "\n",
    "                candidates = self._find_candidates(datapoint_id, candidates, cluster_sizes, debug)\n",
    "                    \n",
    "            debug and print('\\ncandidates:', candidates)\n",
    "            \n",
    "            # break at convergence\n",
    "            if not candidates:      ## [] -> False\n",
    "                debug and print('no more candidates')\n",
    "                break\n",
    "\n",
    "            # proceed in unsafe mode\n",
    "            if not safe_mode and not always_safe and not binary_hartigan:\n",
    "                debug and print('\\nentered in UNSAFE mode')\n",
    "\n",
    "                # store current state for possible rollback\n",
    "                rollback = self.y_pred.copy()\n",
    "\n",
    "                # calculate original cost\n",
    "                original_cost = self._tot_cluster_cost(self.centroids, self.y_pred, debug > 1)\n",
    "                debug and print('original_cost:', original_cost)\n",
    "\n",
    "                # accept all candidates\n",
    "                new_cost, new_centroids, cluster_sizes = self._accept_candidates(candidates, cluster_sizes, debug > 1)\n",
    "                debug and print('new cost:', new_cost)\n",
    "\n",
    "                if new_cost >= original_cost:\n",
    "                    # new clustering is more expensive, proceed in safe mode\n",
    "                    safe_mode = True\n",
    "                    self.y_pred = rollback\n",
    "\n",
    "            # start new condition since safe mode can be entered from unsafe mode\n",
    "            if (safe_mode or always_safe) and not binary_hartigan:\n",
    "                debug and print('\\nentered in SAFE mode')\n",
    "                # the current iteration is safe and not unsafe anymore\n",
    "                self.unsafe_iterations -= 1\n",
    "                self.safe_iterations += 1\n",
    "\n",
    "                # track unchanged clusters and sort candidates by delta cost (most negative first)\n",
    "                unchanged_clusters = list(range(self.k))\n",
    "                sorted_candidates = sorted(candidates.items(), key=lambda e: e[1][1], reverse=True)\n",
    "                debug and print('\\nsorted_candidates:', sorted_candidates)\n",
    "\n",
    "                for datapoint_id, [delta_cost, current_centroid_id, new_centroid_id] in sorted_candidates:\n",
    "\n",
    "                    # if both clusters are still unchanged, accept the candidate\n",
    "                    if current_centroid_id in unchanged_clusters and new_centroid_id in unchanged_clusters:\n",
    "                        debug and print(f'candidate {datapoint_id} moved from {current_centroid_id} to {new_centroid_id}')\n",
    "                        self.y_pred[datapoint_id] = new_centroid_id\n",
    "                        unchanged_clusters.remove(current_centroid_id)\n",
    "                        unchanged_clusters.remove(new_centroid_id)\n",
    "\n",
    "                    # if we cannot operate on any more clusters, break\n",
    "                    if not unchanged_clusters:\n",
    "                        break\n",
    "\n",
    "                new_centroids = self._move_centroids(None, debug > 1)\n",
    "\n",
    "            # proceed in binary-hartigan if needed\n",
    "            elif binary_hartigan:\n",
    "                debug and print('\\nentered in BINARY mode')\n",
    "                self.unsafe_iterations -= 1\n",
    "\n",
    "                # store current state for possible rollback\n",
    "                rollback = self.y_pred.copy()\n",
    "\n",
    "                # calculate original cost\n",
    "                original_cost = self._tot_cluster_cost(self.centroids, self.y_pred, debug > 1)\n",
    "                debug and print('original_cost:', original_cost)\n",
    "\n",
    "                candidates_partition = [candidates]\n",
    "                no_edit = True\n",
    "                while no_edit:\n",
    "                    self.binary_iterations += 1\n",
    "                    debug and print('candidates_partition:', candidates_partition)\n",
    "\n",
    "                    for part in candidates_partition:\n",
    "                        # \"binary\" split\n",
    "                        candidates_items = list(part.items())\n",
    "                        half = len(candidates)//2\n",
    "\n",
    "                        part_1 = dict(candidates_items[:half])                        \n",
    "                        debug and print('part_1:', part_1)\n",
    "                        new_cost, new_centroids, cluster_sizes = self._accept_candidates(part_1, cluster_sizes, debug > 1)\n",
    "                        debug and print('new_cost trying part_1:', new_cost)\n",
    "\n",
    "                        if new_cost >= original_cost:\n",
    "                            # new clustering accepting part_1 is more expensive\n",
    "                            # rollback and try with part_2\n",
    "                            self.y_pred = rollback\n",
    "\n",
    "                            part_2 = dict(candidates_items[half:])\n",
    "                            debug and print('part_2:', part_2)\n",
    "                            new_cost, new_centroids, cluster_sizes = self._accept_candidates(part_2, cluster_sizes, debug > 1)\n",
    "                            debug and print('new_cost trying part_2:', new_cost)\n",
    "                            \n",
    "                            if new_cost >= original_cost:\n",
    "                                # new clustering accepting part_2 is more expensive\n",
    "                                # rollback and proceed with \"binary\" split\n",
    "                                self.y_pred = rollback\n",
    "                            else:\n",
    "                                no_edit = False\n",
    "                                break\n",
    "                        else:\n",
    "                            no_edit = False\n",
    "                            break\n",
    "\n",
    "                        # if no break was encountered, proceed with \"binary\" split\n",
    "                        candidates_partition = [part_1, part_2]\n",
    "\n",
    "            self.centroids = new_centroids\n",
    "\n",
    "    def _hartigan(self, debug=0):\n",
    "        \"\"\"\n",
    "        Hartigan algorithm for k-means clustering.\n",
    "        \"\"\"\n",
    "        debug and print('\\nRunning Hartigan algorithm...')\n",
    "\n",
    "        self.centroids = self._move_centroids(None, debug > 1)\n",
    "\n",
    "        edit = True\n",
    "        while edit:\n",
    "            self.hartigan_iterations += 1\n",
    "\n",
    "            edit = False\n",
    "            \n",
    "            # calculate sizes of clusters and cost of current assignment\n",
    "            cluster_sizes = np.bincount(self.y_pred, minlength=self.k)\n",
    "\n",
    "            for datapoint_id in range(self.n_samples):\n",
    "                debug and print('\\ndatapoint_id:', datapoint_id)\n",
    "                \n",
    "                # find candidates for reassignment of the current datapoint\n",
    "                candidate = self._find_candidates(datapoint_id, {}, cluster_sizes, debug)\n",
    "                debug and print('candidate:', candidate)\n",
    "\n",
    "                if candidate:\n",
    "                    new_cost, new_centroids, cluster_sizes = self._accept_candidates(candidate, cluster_sizes, debug > 1)\n",
    "                    self.centroids = new_centroids\n",
    "                    edit = True\n",
    "                    # the code continues with the next datapoint instead than starting from the first one again\n",
    "\n",
    "\n",
    "    def _move_centroids(self, move_just = None, debug=0):\n",
    "        \"\"\"\n",
    "        Move the centroids to the mean of their cluster.\n",
    "        \"\"\"\n",
    "\n",
    "        debug and print('\\n  moving centroids...')\n",
    "        debug and print('  | y_pred:', self.y_pred)\n",
    "        debug and print('  | data:\\n', self.data)\n",
    "        debug and print('  | centroids_before:\\n', self.centroids)\n",
    "\n",
    "        centroids = np.copy(self.centroids)\n",
    "        \n",
    "        move = move_just if move_just is not None else range(self.k)\n",
    "        debug and print('  | move:', move)\n",
    "        \n",
    "        for centroid_id in move:\n",
    "            \n",
    "            mask = self.y_pred == centroid_id\n",
    "            \n",
    "            # if centroid has no points assigned to it, reassign it randomly\n",
    "            if np.any(mask):\n",
    "                centroids[centroid_id] = np.mean(self.data[mask], axis=0)\n",
    "            else:\n",
    "                debug and print(f\"  Centroid {centroid_id} is empty. Reassigning.\")\n",
    "                new_centroid_id = np.random.choice(self.n_samples)\n",
    "                centroids[centroid_id] = self.data[new_centroid_id]\n",
    "                self.y_pred[new_centroid_id] = centroid_id\n",
    "\n",
    "        debug and print('  centroids_after:\\n', centroids)\n",
    "\n",
    "        return centroids\n",
    "\n",
    "\n",
    "    def _assign_clusters(self, specific_centroids=None, debug=0, init_call=False):\n",
    "        \"\"\"\n",
    "        Assign each data point to the closest centroid.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        specific_centroids : np.ndarray\n",
    "            array of shape (k', d) with specific centroids (of a specific number k') to use for the assignment\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array of length n with cluster assignments for each sample\n",
    "        \"\"\"\n",
    "        \n",
    "        if specific_centroids is None:\n",
    "            specific_centroids = self.centroids\n",
    "        \n",
    "        distances = cdist(self.data, specific_centroids, metric='sqeuclidean')\n",
    "        if init_call:\n",
    "            self.init_norm_calculations += self.n_samples * len(specific_centroids) / (self.k*self.n_samples)\n",
    "        else:\n",
    "            self.norm_calculations += self.n_samples * len(specific_centroids) / (self.k*self.n_samples)\n",
    "        y_pred = np.argmin(distances, axis=1)\n",
    "\n",
    "        debug and print('y_pred:', y_pred)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def _tot_cluster_cost(self, centroids, points_ids, debug=0, init_call=False):\n",
    "        \"\"\"\n",
    "        Compute the overall cost of clustering\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        centroids : np.ndarray\n",
    "            Array of shape ('k, d) with cluster centroids\n",
    "        points_ids : np.ndarray\n",
    "            Array of length n with cluster assignments for each sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Total cost of clustering\n",
    "        \"\"\"\n",
    "        \n",
    "        debug and print('\\n  calculating _tot_cluster_cost')\n",
    "        \n",
    "        total_cost = 0\n",
    "\n",
    "        for centroid_id in range(centroids.shape[0]):\n",
    "            mask = points_ids == centroid_id\n",
    "            points_in_cluster = self.data[mask]\n",
    "\n",
    "            cluster_items = np.where(points_ids == centroid_id)[0]\n",
    "\n",
    "            if len(points_in_cluster) > 0:\n",
    "                # Vectorized computation\n",
    "                cost = np.sum(np.sum((points_in_cluster - centroids[centroid_id])**2, axis=1))\n",
    "                total_cost += cost\n",
    "                if init_call:\n",
    "                    self.init_norm_calculations += len(points_in_cluster) / (self.k*self.n_samples)\n",
    "                else:\n",
    "                    self.norm_calculations += len(points_in_cluster) / (self.k*self.n_samples)\n",
    "\n",
    "            debug and print('  | centroid_id:', centroid_id)\n",
    "            debug and print('  | centroid:', centroids[centroid_id])\n",
    "            debug and print('  | cluster_items:', cluster_items)\n",
    "            debug and print('  | cost:', cost)\n",
    "        \n",
    "        debug and print(f'  total_cost: {total_cost}')\n",
    "        \n",
    "        return total_cost\n",
    "\n",
    "\n",
    "    def _find_candidates(self, datapoint_id, candidates, cluster_sizes, debug=0):\n",
    "        \"\"\"\n",
    "        Find candidates for reassignment of a single datapoint.\n",
    "        \"\"\"\n",
    "\n",
    "        current_centroid_id = self.y_pred[datapoint_id]\n",
    "        current_size = cluster_sizes[current_centroid_id]\n",
    "        \n",
    "        # if current clsuter has only one point, we cannot move it\n",
    "        if current_size <= 1:\n",
    "            return candidates\n",
    "        \n",
    "        prefactor = current_size / (current_size - 1)\n",
    "        current_cost = prefactor * np.sum((self.data[datapoint_id] - self.centroids[current_centroid_id])**2)\n",
    "        self.norm_calculations += 1/(self.k*self.n_samples)\n",
    "\n",
    "        debug and print('current_cost:', current_cost)\n",
    "\n",
    "        # if current_cost is 0, delta_cost will always be positive\n",
    "        if current_cost == 0:\n",
    "            return candidates\n",
    "        \n",
    "        # Vectorized computation for all centroids except the current one\n",
    "        mask = np.arange(self.k) != current_centroid_id\n",
    "        cluster_sizes_masked = cluster_sizes[mask]\n",
    "        valid_centroid_ids = np.where(mask)[0]\n",
    "\n",
    "        # Compute delta costs\n",
    "        delta_costs = (cluster_sizes_masked / (cluster_sizes_masked + 1)) * np.linalg.norm(self.data[datapoint_id] - self.centroids[valid_centroid_ids], axis=1)**2 - current_cost\n",
    "        self.norm_calculations += (len(valid_centroid_ids)) / (self.k*self.n_samples)\n",
    "\n",
    "        # Find the best centroid (most negative delta_cost)\n",
    "        best_idx = np.argmin(delta_costs)\n",
    "        best_delta_cost = delta_costs[best_idx]\n",
    "\n",
    "        if best_delta_cost < 0:\n",
    "            best_centroid_id = valid_centroid_ids[best_idx]\n",
    "            candidates[datapoint_id] = [best_delta_cost, current_centroid_id, best_centroid_id]\n",
    "        \n",
    "        return candidates\n",
    "\n",
    "\n",
    "    def _accept_candidates(self, candidates, cluster_sizes, debug=0):\n",
    "        \"\"\"\n",
    "        Accepts all candidates passed as argument and calculates new total cluster cost.\n",
    "        \"\"\"\n",
    "        # accept all candidates\n",
    "        used_centroids = set()\n",
    "\n",
    "        for datapoint_id, [_, current_centroid_id, new_centroid_id] in candidates.items():\n",
    "\n",
    "            debug and print('candidate:', datapoint_id)\n",
    "\n",
    "            used_centroids.add(current_centroid_id)\n",
    "            used_centroids.add(new_centroid_id)\n",
    "\n",
    "            cluster_sizes[current_centroid_id] -= 1\n",
    "            cluster_sizes[new_centroid_id] += 1\n",
    "\n",
    "            debug and print('y_pred before:', self.y_pred)\n",
    "\n",
    "            # update closest_points_ids assigning datapoint to new_centroid_id\n",
    "            self.y_pred[datapoint_id] = new_centroid_id\n",
    "            debug and print('y_pred after:', self.y_pred)\n",
    "\n",
    "        new_centroids = self._move_centroids(move_just=used_centroids, debug = debug)\n",
    "\n",
    "        return self._tot_cluster_cost(new_centroids, self.y_pred, debug), new_centroids, cluster_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49477796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_index(true_centroids, predicted_centroids):\n",
    "    \"\"\"\n",
    "    Compute the number of centroids that are not matched between true and predicted centroids.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    true_centroids : np.ndarray\n",
    "        Array of shape (k, d) with true centroids\n",
    "    predicted_centroids : np.ndarray\n",
    "        Array of shape (k, d) with predicted centroids\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Number of centroids that are not matched\n",
    "    \"\"\"\n",
    "    \n",
    "    n = true_centroids.shape[0]\n",
    "    assert predicted_centroids.shape[0] == n, \"Number of true and predicted centroids must be the same\"\n",
    "    assert true_centroids.shape[1] == predicted_centroids.shape[1], \"Number of features must be the same\"\n",
    "    \n",
    "    matched = np.zeros(n, dtype=int)\n",
    "    for i in predicted_centroids:\n",
    "        # calculate distance from each true centroid\n",
    "        distances = [np.linalg.norm(i - j) for j in true_centroids]\n",
    "        matched[np.argmin(distances)] += 1\n",
    "\n",
    "    return n - np.count_nonzero(matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce553cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3:\n",
      " [[53920 42968]\n",
      " [52019 42206]\n",
      " [52570 42476]\n",
      " ...\n",
      " [41140 18409]\n",
      " [37752 19891]\n",
      " [40164 17389]]\n",
      "\n",
      "shape: (7500, 2)\n"
     ]
    }
   ],
   "source": [
    "a3 = pd.read_table('data/A-Sets/a3.txt', header=None, sep='   ', engine='python').to_numpy()\n",
    "a3_centroids = pd.read_table('data/A-Sets/truth/a3-ga-cb.txt', header=None, sep='   ', engine='python').to_numpy()\n",
    "print('a3:\\n', a3)\n",
    "print('\\nshape:', a3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a747bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "centroids:\n",
      " [[60204.44594595 45838.85810811]\n",
      " [16837.32666667 53850.44666667]\n",
      " [ 7918.7        12745.85      ]\n",
      " [35557.99375    36911.6       ]\n",
      " [43257.42384106  4639.56953642]\n",
      " [45354.55629139 26049.41059603]\n",
      " [43683.6442953  34812.31543624]\n",
      " [ 5137.56756757 42435.29054054]\n",
      " [42896.24358974 55061.85897436]\n",
      " [26885.93939394 56946.67676768]\n",
      " [59233.96428571  9268.14285714]\n",
      " [39304.82666667 17997.78      ]\n",
      " [17467.25657895 25348.70394737]\n",
      " [50965.56756757 23385.34459459]\n",
      " [56093.0472973  35223.2972973 ]\n",
      " [25553.4        28055.64666667]\n",
      " [ 6707.11072664 26556.56055363]\n",
      " [13268.85234899  4454.70469799]\n",
      " [14748.31460674 38041.12359551]\n",
      " [48428.83233533 10345.30538922]\n",
      " [54059.86885246 43803.73770492]\n",
      " [10207.33552632 50195.21710526]\n",
      " [57981.64772727 16392.65909091]\n",
      " [27143.02666667 15512.88666667]\n",
      " [19659.24666667 60383.73333333]\n",
      " [60241.02       27570.37333333]\n",
      " [19947.48026316  7731.80263158]\n",
      " [10304.96527778 16865.66666667]\n",
      " [36487.2437276  47266.39426523]\n",
      " [57887.9        59083.57333333]\n",
      " [59555.91390728 51744.94701987]\n",
      " [47111.77419355 59097.11612903]\n",
      " [14853.57303371 43396.85393258]\n",
      " [30135.60264901 21236.95364238]\n",
      " [50864.39597315 48768.93959732]\n",
      " [33756.56790123 59024.61728395]\n",
      " [ 6748.60824742 10401.77319588]\n",
      " [17023.30405405 14150.15540541]\n",
      " [43947.89506173 45589.25308642]\n",
      " [ 9591.52941176 37963.96732026]\n",
      " [10474.01333333 59840.45333333]\n",
      " [ 4725.37162162 54020.82432432]\n",
      " [53557.97849462 41447.69892473]\n",
      " [30488.97315436 45033.76510067]\n",
      " [36211.37333333  8492.88      ]\n",
      " [12400.03125    43249.96875   ]\n",
      " [40405.21710526 29101.125     ]\n",
      " [23203.99324324 44859.15540541]\n",
      " [29681.0738255   8824.42281879]\n",
      " [17159.48484848 37552.5       ]]\n",
      "\n",
      "y_pred: [20 42 42 ... 11 11 11]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans('lloyd', 'k-means++')\n",
    "kmeans.fit(a3, 50)\n",
    "print('\\ncentroids:\\n', kmeans.centroids)\n",
    "print('\\ny_pred:', kmeans.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8dd8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "s = []\n",
    "while count <100:\n",
    "    kmeans = KMeans('lloyd', 'greedy-k-means++')\n",
    "    kmeans.fit(a3, 50)\n",
    "    s.append(centroid_index(a3_centroids, kmeans.centroids))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bdeaa97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d86e92ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Centroid index distribution for greedy-k-means++'}>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# count values in list\n",
    "s = pd.Series(s)\n",
    "v = s.value_counts()\n",
    "sorted_v = v.sort_index()\n",
    "sorted_v.plot(kind='bar', title='Centroid index distribution for greedy-k-means++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5e2719d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tests(initialization, algorithm, file_path, k=None, true_centroids=None, sep=' ', n_repetitions=1):\n",
    "    \"\"\"\n",
    "    Perform tests on the initialization and algorithm methods.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    initialization : str\n",
    "        Initialization method\n",
    "    algorithm : str\n",
    "        Algorithm method\n",
    "    file_path : str\n",
    "        Path of the file to load data\n",
    "    k : int\n",
    "        Number of clusters\n",
    "    true_centroids : np.ndarray\n",
    "        Array of shape (k, d) with true centroids\n",
    "    sep : str\n",
    "        Separator for the file\n",
    "    n_repetitions : int\n",
    "        Number of repetitions for each combination of initialization and algorithms\n",
    "    \"\"\"\n",
    "\n",
    "    flag = true_centroids is not None\n",
    "\n",
    "    # look if file in folder exists\n",
    "    name = ''.join(file_path.split('/')[1:]).split('.')[0]\n",
    "    table = pd.DataFrame(columns=['initialization', 'CENTROID_INDEX', 'TIME_EXECUTION', 'FINAL_COST', 'SAFE_ITERATIONS', 'UNSAFE_ITERATIONS', 'LLYOID_ITERATIONS', 'HARTIGAN_ITERATIONS', 'BINARY_ITERATIONS', 'INIT_NORM_CALCULATIONS', 'NORM_CALCULATIONS'], dtype=object)\n",
    "\n",
    "\n",
    "    # load data\n",
    "    data = pd.read_table(file_path, header=None, sep=sep, engine='python').to_numpy(dtype=int)\n",
    "\n",
    "    metrics = {'CENTROID_INDEX': [], 'TIME_EXECUTION': [], 'FINAL_COST': [], 'SAFE_ITERATIONS': [], 'UNSAFE_ITERATIONS': [], 'LLYOID_ITERATIONS': [], 'HARTIGAN_ITERATIONS': [], 'BINARY_ITERATIONS': [], 'INIT_NORM_CALCULATIONS': [], 'NORM_CALCULATIONS': []}\n",
    "\n",
    "    for _ in range(n_repetitions):\n",
    "        kmeans = KMeans(algorithm=algorithm, init=initialization)\n",
    "        start = time.time()\n",
    "        if flag:\n",
    "            kmeans.fit(data, true_centroids.shape[0], debug=0)\n",
    "        else:\n",
    "            kmeans.fit(data, k, debug=0)\n",
    "        end = time.time()\n",
    "\n",
    "        if flag:\n",
    "            metrics['CENTROID_INDEX'].append(centroid_index(true_centroids, kmeans.centroids))\n",
    "        else:\n",
    "            metrics['CENTROID_INDEX'].append(np.nan)\n",
    "\n",
    "        metrics['TIME_EXECUTION'].append(end-start)\n",
    "        metrics['SAFE_ITERATIONS'].append(kmeans.safe_iterations)\n",
    "        metrics['UNSAFE_ITERATIONS'].append(kmeans.unsafe_iterations)\n",
    "        metrics['LLYOID_ITERATIONS'].append(kmeans.lloyd_iterations)\n",
    "        metrics['HARTIGAN_ITERATIONS'].append(kmeans.hartigan_iterations)\n",
    "        metrics['BINARY_ITERATIONS'].append(kmeans.binary_iterations)\n",
    "        metrics['INIT_NORM_CALCULATIONS'].append(kmeans.init_norm_calculations)\n",
    "        metrics['NORM_CALCULATIONS'].append(kmeans.norm_calculations)\n",
    "        metrics['FINAL_COST'].append(kmeans._tot_cluster_cost(kmeans.centroids, kmeans.y_pred))\n",
    "\n",
    "    # add init to dictionary and append to table\n",
    "    metrics['initialization'] = initialization\n",
    "    table = pd.concat([table, pd.DataFrame(metrics)], ignore_index=True)\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def test_all(path, init, algo, rep, k=None, true_centroids=None, sep='   '):\n",
    "    \"\"\"\n",
    "    Tests given algorithms and initializations on a dataset.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path of the file to load\n",
    "    init : list\n",
    "        List of initializations to test\n",
    "    algo : list\n",
    "        List of algorithms to test\n",
    "    rep : int\n",
    "        Number of repetitions for each combination of initialization and algorithms\n",
    "    k : int, optional\n",
    "        Number of clusters. The default is None.\n",
    "    true_centroids : np.ndarray, optional\n",
    "        Array of shape (k, d) with true centroids. The default is None.\n",
    "    sep : str, optional\n",
    "        Separator for the file. The default is '   '.\n",
    "    \"\"\"\n",
    "\n",
    "    assert k is not None or true_centroids is not None, \"Either k or true_centroids must be provided\"\n",
    "    \n",
    "    for a in algo:\n",
    "        for i in init:\n",
    "            return make_tests(i, a, path, k=k, true_centroids=true_centroids, sep=sep, n_repetitions=rep)\n",
    "            print(f'{a} with {i} initialization done')\n",
    "        print()\n",
    "\n",
    "\n",
    "def view_tests(initializations, algorithms, file_path, style=True):\n",
    "    \"\"\"\n",
    "    Perform tests on the initialization and algorithm methods.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    initializations : list\n",
    "        List of initialization methods\n",
    "    algorithms : list\n",
    "        List of algorithm methods\n",
    "    file_path : str\n",
    "        Path of the file to load\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Table with algorithms as columns, each column divided for initializations and rows as: algorithms, centroid index, time execution\n",
    "    \"\"\"\n",
    "\n",
    "    data_frames = {}\n",
    "\n",
    "    # read table from path\n",
    "    name = ''.join(file_path.split('/')[1:]).split('.')[0]\n",
    "\n",
    "\n",
    "    for algorithm in algorithms:\n",
    "        test_data = pd.read_csv(f'tests/{name}-{algorithm}.csv', sep=',', engine='python')\n",
    "\n",
    "        test_data = test_data[test_data[\"initialization\"].isin(initializations)]\n",
    "        data_frames[algorithm] = test_data.groupby(\"initialization\", sort=False).agg(lambda x: list(x))\n",
    "\n",
    "        # compute means/maxs/mins and count\n",
    "        data_frames[algorithm]['count'] = data_frames[algorithm]['TIME_EXECUTION'].apply(lambda x: len(x))\n",
    "        \n",
    "        if not data_frames[algorithm]['CENTROID_INDEX'].isna().any():\n",
    "            data_frames[algorithm]['min_centroid_index'] = data_frames[algorithm]['CENTROID_INDEX'].apply(lambda x: np.min(x))\n",
    "            data_frames[algorithm]['max_centroid_index'] = data_frames[algorithm]['CENTROID_INDEX'].apply(lambda x: np.max(x))\n",
    "            data_frames[algorithm]['mean_centroid_index'] = data_frames[algorithm]['CENTROID_INDEX'].apply(lambda x: np.mean(x))\n",
    "            data_frames[algorithm]['std_centroid_index'] = data_frames[algorithm]['CENTROID_INDEX'].apply(lambda x: np.std(x))\n",
    "        \n",
    "        data_frames[algorithm]['mean_time_execution'] = data_frames[algorithm]['TIME_EXECUTION'].apply(lambda x: np.mean(x))\n",
    "        \n",
    "        data_frames[algorithm]['min_final_cost'] = data_frames[algorithm]['FINAL_COST'].apply(lambda x: np.min(x))\n",
    "        data_frames[algorithm]['mean_final_cost'] = data_frames[algorithm]['FINAL_COST'].apply(lambda x: np.mean(x))\n",
    "        data_frames[algorithm]['max_final_cost'] = data_frames[algorithm]['FINAL_COST'].apply(lambda x: np.max(x))\n",
    "        data_frames[algorithm]['std_final_cost'] = data_frames[algorithm]['FINAL_COST'].apply(lambda x: np.std(x))\n",
    "        \n",
    "        data_frames[algorithm]['mean_safe_iterations'] = data_frames[algorithm]['SAFE_ITERATIONS'].apply(lambda x: np.mean(x))\n",
    "        data_frames[algorithm]['mean_unsafe_iterations'] = data_frames[algorithm]['UNSAFE_ITERATIONS'].apply(lambda x: np.mean(x))\n",
    "        data_frames[algorithm]['mean_lloyd_iterations'] = data_frames[algorithm]['LLYOID_ITERATIONS'].apply(lambda x: np.mean(x))\n",
    "        data_frames[algorithm]['mean_hartigan_iterations'] = data_frames[algorithm]['HARTIGAN_ITERATIONS'].apply(lambda x: np.mean(x))\n",
    "        data_frames[algorithm]['mean_binary_iterations'] = data_frames[algorithm]['BINARY_ITERATIONS'].apply(lambda x: np.mean(x))\n",
    "        data_frames[algorithm]['mean_tot_iterations'] = data_frames[algorithm]['SAFE_ITERATIONS'].apply(lambda x: np.mean(x)) + data_frames[algorithm]['UNSAFE_ITERATIONS'].apply(lambda x: np.mean(x)) + data_frames[algorithm]['LLYOID_ITERATIONS'].apply(lambda x: np.mean(x)) + data_frames[algorithm]['HARTIGAN_ITERATIONS'].apply(lambda x: np.mean(x)) + data_frames[algorithm]['BINARY_ITERATIONS'].apply(lambda x: np.mean(x))\n",
    "        \n",
    "        data_frames[algorithm]['mean_init_norm_calculations'] = data_frames[algorithm]['INIT_NORM_CALCULATIONS'].apply(lambda x: np.mean(x))\n",
    "        data_frames[algorithm]['mean_norm_calculations'] = data_frames[algorithm]['NORM_CALCULATIONS'].apply(lambda x: np.mean(x))\n",
    "        data_frames[algorithm]['mean_tot_norm_calculations'] = data_frames[algorithm]['INIT_NORM_CALCULATIONS'].apply(lambda x: np.mean(x)) + data_frames[algorithm]['NORM_CALCULATIONS'].apply(lambda x: np.mean(x))\n",
    "\n",
    "        # drop columns\n",
    "        data_frames[algorithm].drop(columns=['CENTROID_INDEX', 'TIME_EXECUTION', 'FINAL_COST', 'SAFE_ITERATIONS', 'UNSAFE_ITERATIONS', 'LLYOID_ITERATIONS', 'HARTIGAN_ITERATIONS', 'BINARY_ITERATIONS', 'INIT_NORM_CALCULATIONS', 'NORM_CALCULATIONS'], inplace=True)\n",
    "\n",
    "    result_df = pd.concat(data_frames, axis=0, keys=algorithms, names=[\"Algorithm\", \"Initialization\"])\n",
    "    \n",
    "    def color_specific_columns(x):\n",
    "        df_styled = pd.DataFrame('', index=x.index, columns=x.columns)\n",
    "        df_styled.loc[:, ['min_final_cost', 'mean_tot_iterations', 'mean_tot_norm_calculations', 'mean_centroid_index']] = 'font-weight: bold'\n",
    "        return df_styled\n",
    "\n",
    "    def add_vertical_lines(x):\n",
    "        df_styled = pd.DataFrame('', index=x.index, columns=x.columns)\n",
    "        # Add borders after columns B and C\n",
    "        df_styled.loc[:, ['count', 'min_centroid_index', 'mean_time_execution', 'min_final_cost', 'mean_safe_iterations', 'mean_init_norm_calculations']] = 'border-left: 1px solid gray'\n",
    "        return df_styled\n",
    "\n",
    "    # Apply the styles to the DataFrame\n",
    "    if style:\n",
    "        result_df = result_df.style.apply(color_specific_columns, axis=None).apply(add_vertical_lines, axis=None)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def centroid_id_histogram(initializations, algorithms, file_path, by: Literal['algo', 'init']='algo'):\n",
    "    \"\"\"\n",
    "    Perform tests on the initialization and algorithm methods.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    initializations : list\n",
    "        List of initialization methods\n",
    "    algorithms : list\n",
    "        List of algorithm methods\n",
    "    file_path : str\n",
    "        Path of the file to load\n",
    "    by : {'algo', 'init'}\n",
    "        By which parameter to plot the histograms\n",
    "    \"\"\"\n",
    "\n",
    "    assert by in ['algo', 'init'], \"by must be either 'algo' or 'init'\"\n",
    "\n",
    "    # read table from path\n",
    "    name = ''.join(file_path.split('/')[1:]).split('.')[0]\n",
    "\n",
    "    dict_data = {}\n",
    "    for algorithm in algorithms:\n",
    "        data = pd.read_csv(f'tests/{name}-{algorithm}.csv', sep=',', engine='python')\n",
    "        data = data[data[\"initialization\"].isin(initializations)]\n",
    "        data = data.groupby(\"initialization\", sort=False).agg(lambda x: list(x))\n",
    "        dict_data[algorithm] = data\n",
    "\n",
    "    if by == 'algo':\n",
    "        for algorithm, data in dict_data.items():\n",
    "            fig, ax = plt.subplots(figsize=(15,5))\n",
    "            \n",
    "            centroid_id_dict = {}\n",
    "            for initialization in initializations:\n",
    "                centroid_id_dict[initialization] = data['CENTROID_INDEX'][initialization]\n",
    "            ax.hist(centroid_id_dict.values(), bins=range(12), label=list(centroid_id_dict.keys()), align='left')\n",
    "                \n",
    "            ax.legend()\n",
    "            plt.xticks(range(11))\n",
    "            plt.title(f'{name} - {algorithm}')\n",
    "            plt.show()\n",
    "            \n",
    "    \n",
    "    elif by == 'init':\n",
    "        for initialization in initializations:\n",
    "            # plot a graph for each initialization siding by algorithm\n",
    "            fig, ax = plt.subplots(figsize=(15,5))\n",
    "            \n",
    "            centroid_id_dict = pd.Series()\n",
    "            labels = []\n",
    "            for algorithm, data in dict_data.items():\n",
    "                centroid_id_dict[algorithm] = data.loc[initialization]['CENTROID_INDEX']\n",
    "                labels.append(algorithm)\n",
    "            ax.hist(centroid_id_dict, bins=range(12), label=labels, align='left')\n",
    "            \n",
    "            ax.legend()\n",
    "            plt.xticks(range(11))\n",
    "            plt.title(f'{name} - {initialization}')\n",
    "            plt.show()\n",
    "\n",
    "def min_found(initializations, algorithms, file_path, treshold):\n",
    "    \"\"\"\n",
    "    Returns the percentage of times a minimum (under a certain treshold) was found for each initialization and algorithm.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    initializations : list\n",
    "        List of initialization methods\n",
    "    algorithms : list\n",
    "        List of algorithm methods\n",
    "    file_path : str\n",
    "        Path of the file to load\n",
    "    treshold : float\n",
    "        Treshold for the minimum\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Table with algorithms as columns, each column divided for initializations and rows as: algorithms, centroid index, time execution\n",
    "    \"\"\"\n",
    "    # read table from path\n",
    "    name = ''.join(file_path.split('/')[1:]).split('.')[0]\n",
    "\n",
    "    dict_data = {}\n",
    "    for algorithm in algorithms:\n",
    "        data = pd.read_csv(f'tests/{name}-{algorithm}.csv', sep=',', engine='python')\n",
    "        data = data[data[\"initialization\"].isin(initializations)]\n",
    "        data = data.groupby(\"initialization\", sort=False).agg(lambda x: list(x))\n",
    "        dict_data[algorithm] = data\n",
    "\n",
    "    result_df = pd.DataFrame(columns=initializations, index=algorithms)\n",
    "\n",
    "    for algorithm, data in dict_data.items():\n",
    "        for initialization in initializations:\n",
    "            # calculate the percentage of times a minimum was found\n",
    "            min_found = np.sum(np.array(data['FINAL_COST'][initialization]) <= treshold) / len(data['FINAL_COST'][initialization])\n",
    "            result_df.loc[algorithm, initialization] = min_found\n",
    "    \n",
    "    # all entries must have 3 numbers after the comma\n",
    "    result_df = result_df.map(lambda x: f'{x:.3f}')\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "92704ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stras\\AppData\\Local\\Temp\\ipykernel_34704\\3291845419.py:61: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  table = pd.concat([table, pd.DataFrame(metrics)], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>initialization</th>\n",
       "      <th>CENTROID_INDEX</th>\n",
       "      <th>TIME_EXECUTION</th>\n",
       "      <th>FINAL_COST</th>\n",
       "      <th>SAFE_ITERATIONS</th>\n",
       "      <th>UNSAFE_ITERATIONS</th>\n",
       "      <th>LLYOID_ITERATIONS</th>\n",
       "      <th>HARTIGAN_ITERATIONS</th>\n",
       "      <th>BINARY_ITERATIONS</th>\n",
       "      <th>INIT_NORM_CALCULATIONS</th>\n",
       "      <th>NORM_CALCULATIONS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greedy-k-means++</td>\n",
       "      <td>1</td>\n",
       "      <td>0.727867</td>\n",
       "      <td>3.096170e+10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>275.4</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     initialization CENTROID_INDEX  TIME_EXECUTION    FINAL_COST  \\\n",
       "0  greedy-k-means++              1        0.727867  3.096170e+10   \n",
       "\n",
       "  SAFE_ITERATIONS UNSAFE_ITERATIONS LLYOID_ITERATIONS HARTIGAN_ITERATIONS  \\\n",
       "0               0                 0                11                   0   \n",
       "\n",
       "  BINARY_ITERATIONS  INIT_NORM_CALCULATIONS  NORM_CALCULATIONS  \n",
       "0                 0                   275.4               11.0  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = make_tests('greedy-k-means++', 'lloyd', 'data/A-Sets/a3.txt', 50, a3_centroids, '   ', 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e90f8dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lloyd with k-means++ initialization done\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stras\\AppData\\Local\\Temp\\ipykernel_34704\\677611275.py:61: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  table = pd.concat([table, pd.DataFrame(metrics)], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "a = test_all('data/A-Sets/a3.txt', ['k-means++'], ['lloyd'], 1, k=50, true_centroids=a3_centroids, sep='   ')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd7ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
