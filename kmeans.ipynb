{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Literal\n",
    "\n",
    "class KMeans:\n",
    "    \"\"\"\n",
    "    Perform KMeans clustering on a dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 algorithm : Literal['lloyd', 'extended-hartigan', 'safe-hartigan', 'hartigan'] = 'lloyd',\n",
    "                 init : Literal['random', 'random-data', 'kmeans++', 'greedy'] = 'random',\n",
    "                 seed : Union[int, None] = None):\n",
    "        \"\"\"\n",
    "        Initialize the KMeans object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        algorithm : {'lloyd', 'extended-hartigan', 'safe-hartigan', 'hartigan', 'extended-hartigan-2', 'safe-hartigan-2', 'hartigan-2'}\n",
    "            Algorithm to use. Either 'lloyd' or 'extended-hartigan' or 'safe-hartigan' or 'hartigan' or 'extended-hartigan-2'\n",
    "\n",
    "        init : {'random', 'random-data', 'kmeans++', 'greedy'}\n",
    "            Initialization method. Either 'random' or 'random-data' or 'kmeans++' or 'greedy'\n",
    "\n",
    "        seed : int\n",
    "            Seed for random generator\n",
    "        \"\"\"\n",
    "\n",
    "        assert algorithm in ['lloyd', 'extended-hartigan', 'safe-hartigan', 'hartigan'], \"algorithm must be either 'lloyd', 'extended-hartigan', 'safe-hartigan' or 'hartigan'\"\n",
    "        assert init in ['random', 'random-data', 'kmeans++', 'greedy'], \"init must be either 'random', 'random-data', 'kmeans++' or 'greedy'\"\n",
    "        assert seed is None or isinstance(seed, int), \"seed must be an int or None\"\n",
    "\n",
    "        self.algorithm = algorithm\n",
    "        self.init = init\n",
    "        self.seed = seed\n",
    "\n",
    "        self.data = None\n",
    "        self.k = None\n",
    "        self.centroids = None\n",
    "        self.y_pred = None\n",
    "\n",
    "\n",
    "    def fit(self, data : np.ndarray, k : int, debug=False):\n",
    "        \"\"\"\n",
    "        Fit the model to the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.ndarray\n",
    "            nxd DataFrame of n samples with d features\n",
    "        k : int\n",
    "            Number of clusters\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array of shape (k, d) with cluster centroids\n",
    "        np.ndarray\n",
    "            Array of length n with cluster assignments for each sample\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(data, np.ndarray), \"data must be a numpy array\"\n",
    "        assert len(data.shape) == 2, \"data must be a 2D array\"\n",
    "        assert isinstance(k, int), \"k must be an int\"\n",
    "        assert 0 < k <= len(data), \"k must be at least 0 and at most the number of samples\"\n",
    "        assert isinstance(debug, bool), \"debug must be a boolean\"\n",
    "\n",
    "        self.data = data\n",
    "        self.k = k\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        # initialize centroids\n",
    "        self._init_centroids(debug)\n",
    "        debug and print('initial centroids:\\n', self.centroids)\n",
    "\n",
    "        ## TODO: implement clustering algorithm\n",
    "\n",
    "        if self.algorithm == 'lloyd':\n",
    "            self._lloyd(debug)\n",
    "        elif self.algorithm == 'extended-hartigan':\n",
    "            self._extended_hartigan(always_safe=False, debug=debug)\n",
    "        elif self.algorithm == 'safe-hartigan':\n",
    "            self._extended_hartigan(always_safe=True, debug=debug)\n",
    "        elif self.algorithm == 'hartigan':\n",
    "            return NotImplemented\n",
    "            self._hartigan(debug)\n",
    "        \n",
    "        print('final centroids:\\n', self.centroids)\n",
    "        print('final y_pred:', self.y_pred)\n",
    "\n",
    "\n",
    "    def _init_centroids(self, debug=False):\n",
    "        \"\"\"\n",
    "        Initialize the centroids.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.init == 'random':\n",
    "\n",
    "            # choose k random data points as initial centroids\n",
    "            idx = np.random.choice(self.data.shape[0], self.k, replace=False)\n",
    "            self.centroids = self.data[idx]\n",
    "\n",
    "        elif self.init == 'random-data':\n",
    "\n",
    "            # assign each data point to a random cluster\n",
    "            clusters = np.random.choice(self.k, self.data.shape[0])\n",
    "\n",
    "            # check that at least one point is assigned to each cluster\n",
    "            while len(set(clusters)) < self.k:\n",
    "                clusters = np.random.choice(self.k, self.data.shape[0])\n",
    "            self.y_pred = clusters\n",
    "            self.centroids = self._move_centroids(debug)\n",
    "\n",
    "        elif self.init == 'kmeans++':\n",
    "\n",
    "            # choose first centroid randomly\n",
    "            centroids = np.zeros((self.k, self.data.shape[1]))\n",
    "            centroids[0] = self.data[np.random.choice(self.data.shape[0], 1, replace=False)[0]]\n",
    "            debug and print('centroids:\\n', centroids)\n",
    "\n",
    "            # iterate over remaining k-1 centroids\n",
    "            for i in range(1, self.k):\n",
    "                debug and print('iteration', i)\n",
    "\n",
    "                # calculate distance squared of each point to closest centroid\n",
    "                dist = np.array([min([np.linalg.norm(c-x)**2 for c in centroids[:i]]) for x in self.data])\n",
    "\n",
    "                # probabilities are given by the normalized distance squared\n",
    "                probs = dist / dist.sum()\n",
    "                debug and print('probs:', probs)\n",
    "\n",
    "                # cumulate probabilities\n",
    "                cumprobs = probs.cumsum()\n",
    "\n",
    "                # choose next centroid randomly based on cumulated probabilities\n",
    "                r = np.random.rand()\n",
    "                debug and print('r:', r)\n",
    "                for j, p in enumerate(cumprobs):\n",
    "                    if r < p:\n",
    "                        break\n",
    "\n",
    "                centroids[i] = self.data[j]\n",
    "                debug and print('centroids:\\n', centroids)\n",
    "\n",
    "            self.centroids = centroids\n",
    "\n",
    "        elif self.init == 'greedy':\n",
    "            return NotImplemented\n",
    "\n",
    "\n",
    "    def _lloyd(self, debug=False):\n",
    "        \"\"\"\n",
    "        Lloyd's algorithm for k-means clustering.\n",
    "        \"\"\"\n",
    "\n",
    "        debug and print('\\nRunning Lloyd\\'s algorithm...')\n",
    "\n",
    "        while True:\n",
    "            \n",
    "            debug and print('New iteration')\n",
    "\n",
    "            # assign each data point to the closest centroid\n",
    "            self.y_pred = self._assign_clusters(debug)\n",
    "            debug and print('y_pred:', self.y_pred)\n",
    "\n",
    "            # move centroids to the mean of their cluster\n",
    "            new_centroids = self._move_centroids(debug)\n",
    "\n",
    "            # check for convergence\n",
    "            if np.allclose(self.centroids, new_centroids):\n",
    "                break\n",
    "\n",
    "            self.centroids = new_centroids\n",
    "\n",
    "\n",
    "    def _extended_hartigan(self, always_safe=False, debug=False):\n",
    "        \"\"\"\n",
    "        Extended Hartigan algorithm for k-means clustering.\n",
    "        \"\"\"\n",
    "\n",
    "        debug and print('\\nRunning Extended Hartigan algorithm...')\n",
    "\n",
    "        # first assignment\n",
    "        if self.y_pred is None:\n",
    "            self.y_pred = self._assign_clusters(debug)\n",
    "\n",
    "        # start with unsafe mode    \n",
    "        safe_mode = False\n",
    "\n",
    "        while True:\n",
    "            # create an empty dictionary of new candidates\n",
    "            candidates = {}\n",
    "\n",
    "            # store current state for possible rollback\n",
    "            rollback = self.y_pred.copy()\n",
    "\n",
    "            for datapoint_id in range(len(self.data)):\n",
    "                debug and print('\\ndatapoint_id:', datapoint_id)\n",
    "\n",
    "                # calculate cost of current assignment which remains invariant\n",
    "                current_centroid_id = self.y_pred[datapoint_id]\n",
    "                cluster_size = np.where(self.y_pred == current_centroid_id)[0].shape[0]\n",
    "                prefactor = cluster_size / (cluster_size - 1) if cluster_size > 1 else 0\n",
    "                \n",
    "                current_cost = prefactor * np.linalg.norm(self.data[datapoint_id] - self.centroids[current_centroid_id])**2\n",
    "                debug and print('current_cost:', current_cost)\n",
    "\n",
    "                # iterate only on possible new centroid assignments\n",
    "                for centroid_id in np.setdiff1d(self.y_pred, current_centroid_id):\n",
    "                    delta_cost = self._delta_cost(current_cost, datapoint_id, centroid_id)\n",
    "                    debug and print(f'delta_cost for datapoint {datapoint_id} from centroid {current_centroid_id} to centroid {centroid_id}:', delta_cost)\n",
    "\n",
    "                    # datapoint is a candidate if it reduces the cost\n",
    "                    # if more reassignments reduce the cost, the best one is stored (the one producing the most negfative delta_cost)\n",
    "                    if delta_cost < 0 and (candidates.get(datapoint_id) is None or delta_cost < candidates[datapoint_id][0]):\n",
    "                        candidates[datapoint_id] = [delta_cost, current_centroid_id, centroid_id]\n",
    "\n",
    "            debug and print('\\ncandidates:', candidates)\n",
    "            \n",
    "            # break at convergence\n",
    "            if not candidates:      ## [] -> False\n",
    "                debug and print('no more candidates')\n",
    "                break    \n",
    "\n",
    "            # proceed in unsafe mode\n",
    "            if not safe_mode and not always_safe:\n",
    "                debug and print('entered in unsafe mode')\n",
    "\n",
    "                original_cost = self._tot_cluster_cost(self.centroids, self.y_pred, debug=debug)\n",
    "                debug and print('original_cost:', original_cost)\n",
    "\n",
    "                # accept all candidates\n",
    "                for candidate in candidates.keys():\n",
    "                    print('candidate:', candidate)\n",
    "                    \n",
    "                    [delta_cost, current_centroid_id, new_centroid_id] = candidates[candidate]\n",
    "                    debug and print('y_pred before:', self.y_pred)\n",
    "\n",
    "                    # update closest_points_ids assigning datapoint to new_centroid_id\n",
    "                    self.y_pred[candidate] = new_centroid_id\n",
    "                    debug and print('y_pred after:', self.y_pred)\n",
    "\n",
    "                new_centroids = self._move_centroids(debug)\n",
    "\n",
    "                if self._tot_cluster_cost(new_centroids, self.y_pred, debug=debug) >= original_cost:\n",
    "                    \n",
    "                    # new clustering is more expensive, proceed in safe mode\n",
    "                    safe_mode = True\n",
    "                    self.y_pred = rollback\n",
    "\n",
    "            # start new condition since safe mode can be entered from unsafe mode\n",
    "            if safe_mode or always_safe:\n",
    "                debug and print('entered in safe mode')\n",
    "\n",
    "                unchanged_clusters = list(range(self.k))\n",
    "                for _, [delta_cost, current_centroid_id, new_centroid_id] in sorted(candidates.items(), key=lambda e: e[1][1]):\n",
    "\n",
    "                    # if both clusters are still unchanged, accept the candidate\n",
    "                    if current_centroid_id in unchanged_clusters and new_centroid_id in unchanged_clusters:\n",
    "                        self.y_pred[datapoint_id] = new_centroid_id\n",
    "                        unchanged_clusters.remove(current_centroid_id)\n",
    "                        unchanged_clusters.remove(new_centroid_id)\n",
    "\n",
    "                    # if we cannot operate on any more clusters, break\n",
    "                    if not unchanged_clusters:\n",
    "                        break\n",
    "\n",
    "                new_centroids = self._move_centroids(debug)\n",
    "        \n",
    "            self.centroids = new_centroids\n",
    "\n",
    "        return NotImplemented\n",
    "\n",
    "\n",
    "    def _move_centroids(self, debug=False):\n",
    "        \"\"\"\n",
    "        Move the centroids to the mean of their cluster.\n",
    "        \"\"\"\n",
    "\n",
    "        debug and print('y_pred:', self.y_pred)\n",
    "        debug and print('data:\\n', self.data)\n",
    "        debug and print('centroids_before:\\n', self.centroids)\n",
    "\n",
    "        centroids = np.zeros((self.k, self.data.shape[1]))\n",
    "        for centroid_id in range(self.k):\n",
    "            cluster_points = self.data[self.y_pred == centroid_id]\n",
    "            \n",
    "            # if centroid has no points assigned to it, reassign it randomly\n",
    "            if len(cluster_points) == 0:\n",
    "                debug and print(f\"Centroid {centroid_id} is empty. Reassigning.\")\n",
    "                centroids[centroid_id] = self.data[np.random.choice(len(self.data))]\n",
    "            else:\n",
    "                centroids[centroid_id] = np.mean(cluster_points, axis=0)\n",
    "\n",
    "        debug and print('centroids_after:\\n', centroids)\n",
    "\n",
    "        return centroids\n",
    "\n",
    "\n",
    "    def _assign_clusters(self, debug=False):\n",
    "        \"\"\"\n",
    "        Assign each data point to the closest centroid.\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred = np.zeros(len(self.data), dtype=int)\n",
    "        for i, x in enumerate(self.data):\n",
    "            debug and print(f'{i}:', [np.linalg.norm(x-c)**2 for c in self.centroids])\n",
    "            y_pred[i] = np.argmin([np.linalg.norm(x-c)**2 for c in self.centroids])\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def _delta_cost(self, cost, datapoint_id, centroid_id):\n",
    "        \"\"\"\n",
    "        Compute the change in cost if datapoint is reassigned to centroid_id\n",
    "        \"\"\"\n",
    "\n",
    "        cluster_size = np.where(self.y_pred == centroid_id)[0].shape[0]\n",
    "        prefactor = cluster_size / (cluster_size + 1)\n",
    "\n",
    "        # cost of new assignment\n",
    "        new_cost = prefactor * np.linalg.norm(self.data[datapoint_id] - self.centroids[centroid_id])**2\n",
    "\n",
    "        return new_cost - cost\n",
    "\n",
    "\n",
    "    def _tot_cluster_cost(self, centroids, points_ids, debug=False):\n",
    "        \"\"\"\n",
    "        Compute the overall cost of clustering\n",
    "        \"\"\"\n",
    "        debug and print('\\ninside _tot_cluster_cost')\n",
    "        \n",
    "        partial_sum = []\n",
    "        for centroid_id in range(centroids.shape[0]):\n",
    "            cluster_items = np.where(points_ids == centroid_id)[0]\n",
    "            partial_sum.append(np.sum(np.square(self.data[cluster_items] - self.centroids[centroid_id])))\n",
    "\n",
    "            debug and print('| centroid_id:', centroid_id)\n",
    "            debug and print('| centroid:\\n', centroids[centroid_id])\n",
    "            debug and print('| cluster_items:', cluster_items)\n",
    "            debug and print('| partial_sum:', partial_sum)\n",
    "        \n",
    "        debug and print('_tot_cluster_cost:', np.sum(partial_sum))\n",
    "\n",
    "        return np.sum(partial_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true : np.ndarray, y_pred : np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of the clustering.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        True labels of the samples\n",
    "    y_pred : np.ndarray\n",
    "        Predicted labels of the samples\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Accuracy of the clustering through Hungarian algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(y_true, np.ndarray), \"y_true must be a numpy array\"\n",
    "    assert isinstance(y_pred, np.ndarray), \"y_pred must be a numpy array\"\n",
    "\n",
    "    # create C matrix\n",
    "    n_classes = max(max(y_true), max(y_pred)) + 1\n",
    "    C = np.zeros((n_classes, n_classes), dtype=int)\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        C[true_label, pred_label] += 1\n",
    "    \n",
    "    # Solve assignment problem\n",
    "    row_ind, col_ind = linear_sum_assignment(-C)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    matched = C[row_ind, col_ind].sum(axis=0)\n",
    "    accuracy = matched / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centroids:\n",
      " [[1. 2.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "iteration 1\n",
      "probs: [0.         0.00699301 0.17482517 0.25174825 0.56643357]\n",
      "r: 0.11165717709251399\n",
      "centroids:\n",
      " [[ 1.  2.]\n",
      " [11. 12.]\n",
      " [ 0.  0.]]\n",
      "iteration 2\n",
      "probs: [0.         0.05555556 0.         0.05555556 0.88888889]\n",
      "r: 0.9960458970299656\n",
      "centroids:\n",
      " [[ 1.  2.]\n",
      " [11. 12.]\n",
      " [19. 20.]]\n",
      "initial centroids:\n",
      " [[ 1.  2.]\n",
      " [11. 12.]\n",
      " [19. 20.]]\n",
      "Running Lloyd's algorithm...\n",
      "New iteration\n",
      "[np.float64(0.0), np.float64(200.00000000000003), np.float64(648.0)]\n",
      "[np.float64(8.000000000000002), np.float64(128.00000000000003), np.float64(512.0000000000001)]\n",
      "[np.float64(200.00000000000003), np.float64(0.0), np.float64(128.00000000000003)]\n",
      "[np.float64(287.99999999999994), np.float64(8.000000000000002), np.float64(71.99999999999999)]\n",
      "[np.float64(648.0), np.float64(128.00000000000003), np.float64(0.0)]\n",
      "y_pred: [0 0 1 1 2]\n",
      "y_pred: [0 0 1 1 2]\n",
      "data:\n",
      " [[ 1  2]\n",
      " [ 3  4]\n",
      " [11 12]\n",
      " [13 14]\n",
      " [19 20]]\n",
      "centroids_before:\n",
      " [[ 1.  2.]\n",
      " [11. 12.]\n",
      " [19. 20.]]\n",
      "centroids_after:\n",
      " [[ 2.  3.]\n",
      " [12. 13.]\n",
      " [19. 20.]]\n",
      "New iteration\n",
      "[np.float64(2.0000000000000004), np.float64(241.99999999999997), np.float64(648.0)]\n",
      "[np.float64(2.0000000000000004), np.float64(162.0), np.float64(512.0000000000001)]\n",
      "[np.float64(162.0), np.float64(2.0000000000000004), np.float64(128.00000000000003)]\n",
      "[np.float64(241.99999999999997), np.float64(2.0000000000000004), np.float64(71.99999999999999)]\n",
      "[np.float64(577.9999999999999), np.float64(98.0), np.float64(0.0)]\n",
      "y_pred: [0 0 1 1 2]\n",
      "y_pred: [0 0 1 1 2]\n",
      "data:\n",
      " [[ 1  2]\n",
      " [ 3  4]\n",
      " [11 12]\n",
      " [13 14]\n",
      " [19 20]]\n",
      "centroids_before:\n",
      " [[ 2.  3.]\n",
      " [12. 13.]\n",
      " [19. 20.]]\n",
      "centroids_after:\n",
      " [[ 2.  3.]\n",
      " [12. 13.]\n",
      " [19. 20.]]\n",
      "final centroids:\n",
      " [[ 2.  3.]\n",
      " [12. 13.]\n",
      " [19. 20.]]\n",
      "final y_pred: [0 0 1 1 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2], [3, 4], [11, 12], [13, 14], [19, 20]])\n",
    "\n",
    "kmeans = KMeans(algorithm='lloyd', init='kmeans++')\n",
    "kmeans.fit(a, 3, True)\n",
    "accuracy(np.array([0, 0, 1, 1, 2]), kmeans.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial centroids:\n",
      " [[13 14]\n",
      " [ 3  4]\n",
      " [19 20]]\n",
      "\n",
      "Running Extended Hartigan algorithm...\n",
      "0: [np.float64(287.99999999999994), np.float64(8.000000000000002), np.float64(648.0)]\n",
      "1: [np.float64(200.00000000000003), np.float64(0.0), np.float64(512.0000000000001)]\n",
      "2: [np.float64(8.000000000000002), np.float64(128.00000000000003), np.float64(128.00000000000003)]\n",
      "3: [np.float64(0.0), np.float64(200.00000000000003), np.float64(71.99999999999999)]\n",
      "4: [np.float64(71.99999999999999), np.float64(512.0000000000001), np.float64(0.0)]\n",
      "\n",
      "datapoint_id: 0\n",
      "current_cost: 16.000000000000004\n",
      "delta_cost for datapoint 0 from centroid 1 to centroid 0: 175.99999999999994\n",
      "delta_cost for datapoint 0 from centroid 1 to centroid 2: 308.0\n",
      "\n",
      "datapoint_id: 1\n",
      "current_cost: 0.0\n",
      "delta_cost for datapoint 1 from centroid 1 to centroid 0: 133.33333333333334\n",
      "delta_cost for datapoint 1 from centroid 1 to centroid 2: 256.00000000000006\n",
      "\n",
      "datapoint_id: 2\n",
      "current_cost: 16.000000000000004\n",
      "delta_cost for datapoint 2 from centroid 0 to centroid 1: 69.33333333333334\n",
      "delta_cost for datapoint 2 from centroid 0 to centroid 2: 48.000000000000014\n",
      "\n",
      "datapoint_id: 3\n",
      "current_cost: 0.0\n",
      "delta_cost for datapoint 3 from centroid 0 to centroid 1: 133.33333333333334\n",
      "delta_cost for datapoint 3 from centroid 0 to centroid 2: 35.99999999999999\n",
      "\n",
      "datapoint_id: 4\n",
      "current_cost: 0.0\n",
      "delta_cost for datapoint 4 from centroid 2 to centroid 0: 47.999999999999986\n",
      "delta_cost for datapoint 4 from centroid 2 to centroid 1: 341.33333333333337\n",
      "\n",
      "candidates: {}\n",
      "no more candidates\n",
      "final centroids:\n",
      " [[13 14]\n",
      " [ 3  4]\n",
      " [19 20]]\n",
      "final y_pred: [1 1 0 0 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2], [3, 4], [11, 12], [13, 14], [19, 20]])\n",
    "\n",
    "kmeans = KMeans(algorithm='extended-hartigan', init='random')\n",
    "kmeans.fit(a, 3, True)\n",
    "accuracy(np.array([0, 0, 1, 1, 2]), kmeans.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial centroids:\n",
      " [[19 20]\n",
      " [11 12]\n",
      " [ 1  2]]\n",
      "\n",
      "Running Extended Hartigan algorithm...\n",
      "0: [np.float64(648.0), np.float64(200.00000000000003), np.float64(0.0)]\n",
      "1: [np.float64(512.0000000000001), np.float64(128.00000000000003), np.float64(8.000000000000002)]\n",
      "2: [np.float64(128.00000000000003), np.float64(0.0), np.float64(200.00000000000003)]\n",
      "3: [np.float64(71.99999999999999), np.float64(8.000000000000002), np.float64(287.99999999999994)]\n",
      "4: [np.float64(0.0), np.float64(128.00000000000003), np.float64(648.0)]\n",
      "\n",
      "datapoint_id: 0\n",
      "current_cost: 0.0\n",
      "delta_cost for datapoint 0 from centroid 2 to centroid 0: 324.0\n",
      "delta_cost for datapoint 0 from centroid 2 to centroid 1: 133.33333333333334\n",
      "\n",
      "datapoint_id: 1\n",
      "current_cost: 16.000000000000004\n",
      "delta_cost for datapoint 1 from centroid 2 to centroid 0: 240.00000000000006\n",
      "delta_cost for datapoint 1 from centroid 2 to centroid 1: 69.33333333333334\n",
      "\n",
      "datapoint_id: 2\n",
      "current_cost: 0.0\n",
      "delta_cost for datapoint 2 from centroid 1 to centroid 0: 64.00000000000001\n",
      "delta_cost for datapoint 2 from centroid 1 to centroid 2: 133.33333333333334\n",
      "\n",
      "datapoint_id: 3\n",
      "current_cost: 16.000000000000004\n",
      "delta_cost for datapoint 3 from centroid 1 to centroid 0: 19.99999999999999\n",
      "delta_cost for datapoint 3 from centroid 1 to centroid 2: 175.99999999999994\n",
      "\n",
      "datapoint_id: 4\n",
      "current_cost: 0.0\n",
      "delta_cost for datapoint 4 from centroid 0 to centroid 1: 85.33333333333334\n",
      "delta_cost for datapoint 4 from centroid 0 to centroid 2: 432.0\n",
      "\n",
      "candidates: {}\n",
      "no more candidates\n",
      "final centroids:\n",
      " [[19 20]\n",
      " [11 12]\n",
      " [ 1  2]]\n",
      "final y_pred: [2 2 1 1 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2], [3, 4], [11, 12], [13, 14], [19, 20]])\n",
    "\n",
    "kmeans = KMeans(algorithm='safe-hartigan', init='random')\n",
    "kmeans.fit(a, 3, True)\n",
    "accuracy(np.array([0, 0, 1, 1, 2]), kmeans.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [2., 2.]])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0.,0.], [1.,1.], [2.,2.]])\n",
    "\n",
    "# only positive entries\n",
    "print(np.all(a != 0, axis=1))\n",
    "a[np.all(a != 0, axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans as KMeans_sklearn\n",
    "\n",
    "KMeans = KMeans_sklearn()\n",
    "KMeans.fit = KMeans_sklearn.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
