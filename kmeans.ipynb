{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Literal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    \"\"\"\n",
    "    Perform KMeans clustering on a dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 algorithm : Literal['lloyd', 'extended-hartigan', 'safe-hartigan', 'hartigan'] = 'lloyd',\n",
    "                 init : Literal['random', 'random-data', 'kmeans++', 'greedy'] = 'random',\n",
    "                 seed : Union[int, None] = None):\n",
    "        \"\"\"\n",
    "        Initialize the KMeans object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        algorithm : {'lloyd', 'extended-hartigan', 'safe-hartigan', 'hartigan', 'extended-hartigan-2', 'safe-hartigan-2', 'hartigan-2'}\n",
    "            Algorithm to use. Either 'lloyd' or 'extended-hartigan' or 'safe-hartigan' or 'hartigan' or 'extended-hartigan-2'\n",
    "\n",
    "        init : {'random', 'random-data', 'kmeans++', 'greedy'}\n",
    "            Initialization method. Either 'random' or 'random-data' or 'kmeans++' or 'greedy'\n",
    "\n",
    "        seed : int\n",
    "            Seed for random generator\n",
    "        \"\"\"\n",
    "\n",
    "        assert algorithm in ['lloyd', 'extended-hartigan', 'safe-hartigan', 'hartigan'], \"algorithm must be either 'lloyd', 'extended-hartigan', 'safe-hartigan' or 'hartigan'\"\n",
    "        assert init in ['random', 'random-data', 'kmeans++', 'greedy'], \"init must be either 'random', 'random-data', 'kmeans++' or 'greedy'\"\n",
    "        assert seed is None or isinstance(seed, int), \"seed must be an int or None\"\n",
    "\n",
    "        self.algorithm = algorithm\n",
    "        self.init = init\n",
    "        self.seed = seed\n",
    "\n",
    "        self.data = None\n",
    "        self.k = None\n",
    "        self.centroids = None\n",
    "        self.y_pred = None\n",
    "\n",
    "\n",
    "    def fit(self, data : np.ndarray, k : int, debug=False):\n",
    "        \"\"\"\n",
    "        Fit the model to the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.ndarray\n",
    "            nxd DataFrame of n samples with d features\n",
    "        k : int\n",
    "            Number of clusters\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array of shape (k, d) with cluster centroids\n",
    "        np.ndarray\n",
    "            Array of length n with cluster assignments for each sample\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(data, np.ndarray), \"data must be a numpy array\"\n",
    "        assert len(data.shape) == 2, \"data must be a 2D array\"\n",
    "        assert isinstance(k, int), \"k must be an int\"\n",
    "        assert 0 < k <= len(data), \"k must be at least 0 and at most the number of samples\"\n",
    "        assert isinstance(debug, bool), \"debug must be a boolean\"\n",
    "\n",
    "        self.data = data\n",
    "        self.k = k\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        # initialize centroids\n",
    "        self._init_centroids(debug)\n",
    "        debug and print('initial centroids:\\n', self.centroids)\n",
    "\n",
    "        ## TODO: implement clustering algorithm\n",
    "\n",
    "        if self.algorithm == 'lloyd':\n",
    "            self._lloyd(debug)\n",
    "        elif self.algorithm == 'extended-hartigan':\n",
    "            self._extended_hartigan(always_safe=False, debug=debug)\n",
    "        elif self.algorithm == 'safe-hartigan':\n",
    "            self._extended_hartigan(always_safe=True, debug=debug)\n",
    "        elif self.algorithm == 'hartigan':\n",
    "            return NotImplemented\n",
    "            self._hartigan(debug)\n",
    "        \n",
    "        print('final centroids:\\n', self.centroids)\n",
    "        print('final y_pred:', self.y_pred)\n",
    "\n",
    "\n",
    "    def _init_centroids(self, debug=False):\n",
    "        \"\"\"\n",
    "        Initialize the centroids.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.init == 'random':\n",
    "\n",
    "            # choose k random data points as initial centroids\n",
    "            idx = np.random.choice(self.data.shape[0], self.k, replace=False)\n",
    "            self.centroids = self.data[idx]\n",
    "\n",
    "        elif self.init == 'random-data':\n",
    "\n",
    "            # assign each data point to a random cluster\n",
    "            clusters = np.random.choice(self.k, self.data.shape[0])\n",
    "\n",
    "            # check that at least one point is assigned to each cluster\n",
    "            while len(set(clusters)) < self.k:\n",
    "                clusters = np.random.choice(self.k, self.data.shape[0])\n",
    "            self.y_pred = clusters\n",
    "            self.centroids = self._move_centroids(debug)\n",
    "\n",
    "        elif self.init == 'kmeans++':\n",
    "\n",
    "            # choose first centroid randomly\n",
    "            centroids = np.zeros((self.k, self.data.shape[1]))\n",
    "            centroids[0] = self.data[np.random.choice(self.data.shape[0], 1, replace=False)[0]]\n",
    "            debug and print('centroids:\\n', centroids)\n",
    "\n",
    "            # iterate over remaining k-1 centroids\n",
    "            for i in range(1, self.k):\n",
    "                debug and print('iteration', i)\n",
    "\n",
    "                # calculate distance squared of each point to closest centroid\n",
    "                dist = np.array([min([np.linalg.norm(c-x)**2 for c in centroids[:i]]) for x in self.data])\n",
    "\n",
    "                # probabilities are given by the normalized distance squared\n",
    "                probs = dist / dist.sum()\n",
    "                debug and print('probs:', probs)\n",
    "\n",
    "                # cumulate probabilities\n",
    "                cumprobs = probs.cumsum()\n",
    "\n",
    "                # choose next centroid randomly based on cumulated probabilities\n",
    "                r = np.random.rand()\n",
    "                debug and print('r:', r)\n",
    "                for j, p in enumerate(cumprobs):\n",
    "                    if r < p:\n",
    "                        break\n",
    "\n",
    "                centroids[i] = self.data[j]\n",
    "                debug and print('centroids:\\n', centroids)\n",
    "\n",
    "            self.centroids = centroids\n",
    "\n",
    "        elif self.init == 'greedy':\n",
    "            return NotImplemented\n",
    "\n",
    "\n",
    "    def _lloyd(self, debug=False):\n",
    "        \"\"\"\n",
    "        Lloyd's algorithm for k-means clustering.\n",
    "        \"\"\"\n",
    "\n",
    "        debug and print('\\nRunning Lloyd\\'s algorithm...')\n",
    "\n",
    "        while True:\n",
    "            \n",
    "            debug and print('New iteration')\n",
    "\n",
    "            # assign each data point to the closest centroid\n",
    "            self.y_pred = self._assign_clusters(debug)\n",
    "            debug and print('y_pred:', self.y_pred)\n",
    "\n",
    "            # move centroids to the mean of their cluster\n",
    "            new_centroids = self._move_centroids(debug)\n",
    "\n",
    "            # check for convergence\n",
    "            if np.allclose(self.centroids, new_centroids):\n",
    "                break\n",
    "\n",
    "            self.centroids = new_centroids\n",
    "\n",
    "\n",
    "    def _extended_hartigan(self, always_safe=False, debug=False):\n",
    "        \"\"\"\n",
    "        Extended Hartigan algorithm for k-means clustering.\n",
    "        \"\"\"\n",
    "\n",
    "        debug and print('\\nRunning Extended Hartigan algorithm...')\n",
    "\n",
    "        # first assignment\n",
    "        if self.y_pred is None:\n",
    "            self.y_pred = self._assign_clusters(debug)\n",
    "\n",
    "        while True:\n",
    "            # start with unsafe mode    \n",
    "            safe_mode = False\n",
    "\n",
    "            # create an empty dictionary of new candidates\n",
    "            candidates = {}\n",
    "\n",
    "            for datapoint_id in range(len(self.data)):\n",
    "                debug and print('\\ndatapoint_id:', datapoint_id)\n",
    "\n",
    "                # calculate cost of current assignment which remains invariant\n",
    "                current_centroid_id = self.y_pred[datapoint_id]\n",
    "                cluster_size = np.where(self.y_pred == current_centroid_id)[0].shape[0]\n",
    "                prefactor = cluster_size / (cluster_size - 1) if cluster_size > 1 else 0\n",
    "                \n",
    "                current_cost = prefactor * np.linalg.norm(self.data[datapoint_id] - self.centroids[current_centroid_id])**2\n",
    "                debug and print('current_cost:', current_cost)\n",
    "\n",
    "                # if current_cost is 0, delta_cost will always be positive\n",
    "                if current_cost == 0:\n",
    "                    continue\n",
    "\n",
    "                # iterate only on possible new centroid assignments\n",
    "                for centroid_id in np.setdiff1d(self.y_pred, current_centroid_id):\n",
    "                    delta_cost = self._delta_cost(current_cost, datapoint_id, centroid_id)\n",
    "                    debug and print(f'delta_cost for datapoint {datapoint_id} from centroid {current_centroid_id} to centroid {centroid_id}:', delta_cost)\n",
    "\n",
    "                    # datapoint is a candidate if it reduces the cost\n",
    "                    # if more reassignments reduce the cost, the best one is stored (the one producing the most negfative delta_cost)\n",
    "                    if delta_cost < 0 and (candidates.get(datapoint_id) is None or delta_cost < candidates[datapoint_id][0]):\n",
    "                        candidates[datapoint_id] = [delta_cost, current_centroid_id, centroid_id]\n",
    "\n",
    "            debug and print('\\ncandidates:', candidates)\n",
    "            \n",
    "            # break at convergence\n",
    "            if not candidates:      ## [] -> False\n",
    "                debug and print('no more candidates')\n",
    "                break    \n",
    "\n",
    "            # proceed in unsafe mode\n",
    "            if not safe_mode and not always_safe:\n",
    "                debug and print('\\nentered in UNSAFE mode')\n",
    "\n",
    "                # store current state for possible rollback\n",
    "                rollback = self.y_pred.copy()\n",
    "\n",
    "                # calculate original cost\n",
    "                debug and print('\\ncalculating original cost...')\n",
    "                original_cost = self._tot_cluster_cost(self.centroids, self.y_pred, debug=debug)\n",
    "                debug and print('original_cost:', original_cost)\n",
    "\n",
    "                # accept all candidates\n",
    "                for candidate in candidates.keys():\n",
    "                    debug and print('candidate:', candidate)\n",
    "                    \n",
    "                    [delta_cost, current_centroid_id, new_centroid_id] = candidates[candidate]\n",
    "                    debug and print('y_pred before:', self.y_pred)\n",
    "\n",
    "                    # update closest_points_ids assigning datapoint to new_centroid_id\n",
    "                    self.y_pred[candidate] = new_centroid_id\n",
    "                    debug and print('y_pred after:', self.y_pred)\n",
    "\n",
    "                new_centroids = self._move_centroids(debug)\n",
    "\n",
    "                debug and print('\\ncalculating new cost...')\n",
    "                if self._tot_cluster_cost(new_centroids, self.y_pred, debug=debug) >= original_cost:\n",
    "                    \n",
    "                    # new clustering is more expensive, proceed in safe mode\n",
    "                    safe_mode = True\n",
    "                    self.y_pred = rollback\n",
    "\n",
    "            # start new condition since safe mode can be entered from unsafe mode\n",
    "            if safe_mode or always_safe:\n",
    "                debug and print('\\nentered in SAFE mode')\n",
    "\n",
    "                unchanged_clusters = list(range(self.k))\n",
    "                for _, [delta_cost, current_centroid_id, new_centroid_id] in sorted(candidates.items(), key=lambda e: e[1][1]):\n",
    "\n",
    "                    # if both clusters are still unchanged, accept the candidate\n",
    "                    if current_centroid_id in unchanged_clusters and new_centroid_id in unchanged_clusters:\n",
    "                        self.y_pred[datapoint_id] = new_centroid_id\n",
    "                        unchanged_clusters.remove(current_centroid_id)\n",
    "                        unchanged_clusters.remove(new_centroid_id)\n",
    "\n",
    "                    # if we cannot operate on any more clusters, break\n",
    "                    if not unchanged_clusters:\n",
    "                        break\n",
    "\n",
    "                new_centroids = self._move_centroids(debug)\n",
    "        \n",
    "            self.centroids = new_centroids\n",
    "\n",
    "        return NotImplemented\n",
    "\n",
    "\n",
    "    def _move_centroids(self, debug=False):\n",
    "        \"\"\"\n",
    "        Move the centroids to the mean of their cluster.\n",
    "        \"\"\"\n",
    "\n",
    "        debug and print('\\n  moving centroids...')\n",
    "        debug and print('  | y_pred:', self.y_pred)\n",
    "        debug and print('  | data:\\n', self.data)\n",
    "        debug and print('  | centroids_before:\\n', self.centroids)\n",
    "\n",
    "        centroids = np.zeros((self.k, self.data.shape[1]))\n",
    "        for centroid_id in range(self.k):\n",
    "            cluster_points = self.data[self.y_pred == centroid_id]\n",
    "            \n",
    "            # if centroid has no points assigned to it, reassign it randomly\n",
    "            if len(cluster_points) == 0:\n",
    "                debug and print(f\"  Centroid {centroid_id} is empty. Reassigning.\")\n",
    "                centroids[centroid_id] = self.data[np.random.choice(len(self.data))]\n",
    "            else:\n",
    "                centroids[centroid_id] = np.mean(cluster_points, axis=0)\n",
    "\n",
    "        debug and print('  centroids_after:\\n', centroids)\n",
    "\n",
    "        return centroids\n",
    "\n",
    "\n",
    "    def _assign_clusters(self, debug=False):\n",
    "        \"\"\"\n",
    "        Assign each data point to the closest centroid.\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred = np.zeros(len(self.data), dtype=int)\n",
    "        for i, x in enumerate(self.data):\n",
    "            debug and print(f'{i}:', [np.linalg.norm(x-c)**2 for c in self.centroids])\n",
    "            y_pred[i] = np.argmin([np.linalg.norm(x-c)**2 for c in self.centroids])\n",
    "\n",
    "        debug and print('y_pred:', y_pred)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def _delta_cost(self, cost, datapoint_id, centroid_id):\n",
    "        \"\"\"\n",
    "        Compute the change in cost if datapoint is reassigned to centroid_id\n",
    "        \"\"\"\n",
    "\n",
    "        cluster_size = np.where(self.y_pred == centroid_id)[0].shape[0]\n",
    "        prefactor = cluster_size / (cluster_size + 1)\n",
    "\n",
    "        # cost of new assignment\n",
    "        new_cost = prefactor * np.linalg.norm(self.data[datapoint_id] - self.centroids[centroid_id])**2\n",
    "\n",
    "        return new_cost - cost\n",
    "\n",
    "\n",
    "    def _tot_cluster_cost(self, centroids, points_ids, debug=False):\n",
    "        \"\"\"\n",
    "        Compute the overall cost of clustering\n",
    "        \"\"\"\n",
    "        \n",
    "        debug and print('  calculating _tot_cluster_cost')\n",
    "        \n",
    "        partial_sum = []\n",
    "        for centroid_id in range(centroids.shape[0]):\n",
    "            cluster_items = np.where(points_ids == centroid_id)[0]\n",
    "            partial_sum.append(np.sum(np.square(self.data[cluster_items] - self.centroids[centroid_id])))\n",
    "\n",
    "            debug and print('  | centroid_id:', centroid_id)\n",
    "            debug and print('  | centroid:', centroids[centroid_id])\n",
    "            debug and print('  | cluster_items:', cluster_items)\n",
    "            debug and print('  | partial_sum:', partial_sum)\n",
    "        \n",
    "        debug and print('  _tot_cluster_cost:', np.sum(partial_sum))\n",
    "\n",
    "        return np.sum(partial_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true : np.ndarray, y_pred : np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of the clustering.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        True labels of the samples\n",
    "    y_pred : np.ndarray\n",
    "        Predicted labels of the samples\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Accuracy of the clustering through Hungarian algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(y_true, np.ndarray), \"y_true must be a numpy array\"\n",
    "    assert isinstance(y_pred, np.ndarray), \"y_pred must be a numpy array\"\n",
    "\n",
    "    # create C matrix\n",
    "    n_classes = max(max(y_true), max(y_pred)) + 1\n",
    "    C = np.zeros((n_classes, n_classes), dtype=int)\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        C[true_label, pred_label] += 1\n",
    "    \n",
    "    # Solve assignment problem\n",
    "    row_ind, col_ind = linear_sum_assignment(-C)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    matched = C[row_ind, col_ind].sum(axis=0)\n",
    "    accuracy = matched / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial centroids:\n",
      " [[19 20]\n",
      " [ 1  2]\n",
      " [11 12]]\n",
      "\n",
      "Running Extended Hartigan algorithm...\n",
      "0: [np.float64(648.0), np.float64(0.0), np.float64(200.00000000000003)]\n",
      "1: [np.float64(512.0000000000001), np.float64(8.000000000000002), np.float64(128.00000000000003)]\n",
      "2: [np.float64(128.00000000000003), np.float64(200.00000000000003), np.float64(0.0)]\n",
      "3: [np.float64(71.99999999999999), np.float64(287.99999999999994), np.float64(8.000000000000002)]\n",
      "4: [np.float64(0.0), np.float64(648.0), np.float64(128.00000000000003)]\n",
      "y_pred: [1 1 2 2 0]\n",
      "\n",
      "datapoint_id: 0\n",
      "current_cost: 0.0\n",
      "\n",
      "datapoint_id: 1\n",
      "current_cost: 16.000000000000004\n",
      "delta_cost for datapoint 1 from centroid 1 to centroid 0: 240.00000000000006\n",
      "delta_cost for datapoint 1 from centroid 1 to centroid 2: 69.33333333333334\n",
      "\n",
      "datapoint_id: 2\n",
      "current_cost: 0.0\n",
      "\n",
      "datapoint_id: 3\n",
      "current_cost: 16.000000000000004\n",
      "delta_cost for datapoint 3 from centroid 2 to centroid 0: 19.99999999999999\n",
      "delta_cost for datapoint 3 from centroid 2 to centroid 1: 175.99999999999994\n",
      "\n",
      "datapoint_id: 4\n",
      "current_cost: 0.0\n",
      "\n",
      "candidates: {}\n",
      "no more candidates\n",
      "final centroids:\n",
      " [[19 20]\n",
      " [ 1  2]\n",
      " [11 12]]\n",
      "final y_pred: [1 1 2 2 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debug\n",
    "a = np.array([[1, 2], [3, 4], [11, 12], [13, 14], [19, 20]])\n",
    "\n",
    "kmeans = KMeans(algorithm='extended-hartigan', init='random')\n",
    "kmeans.fit(a, 3, True)\n",
    "accuracy(np.array([0, 0, 1, 1, 2]), kmeans.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centroids:\n",
      " [[1. 2.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "iteration 1\n",
      "probs: [0.         0.00699301 0.17482517 0.25174825 0.56643357]\n",
      "r: 0.11165717709251399\n",
      "centroids:\n",
      " [[ 1.  2.]\n",
      " [11. 12.]\n",
      " [ 0.  0.]]\n",
      "iteration 2\n",
      "probs: [0.         0.05555556 0.         0.05555556 0.88888889]\n",
      "r: 0.9960458970299656\n",
      "centroids:\n",
      " [[ 1.  2.]\n",
      " [11. 12.]\n",
      " [19. 20.]]\n",
      "initial centroids:\n",
      " [[ 1.  2.]\n",
      " [11. 12.]\n",
      " [19. 20.]]\n",
      "Running Lloyd's algorithm...\n",
      "New iteration\n",
      "[np.float64(0.0), np.float64(200.00000000000003), np.float64(648.0)]\n",
      "[np.float64(8.000000000000002), np.float64(128.00000000000003), np.float64(512.0000000000001)]\n",
      "[np.float64(200.00000000000003), np.float64(0.0), np.float64(128.00000000000003)]\n",
      "[np.float64(287.99999999999994), np.float64(8.000000000000002), np.float64(71.99999999999999)]\n",
      "[np.float64(648.0), np.float64(128.00000000000003), np.float64(0.0)]\n",
      "y_pred: [0 0 1 1 2]\n",
      "y_pred: [0 0 1 1 2]\n",
      "data:\n",
      " [[ 1  2]\n",
      " [ 3  4]\n",
      " [11 12]\n",
      " [13 14]\n",
      " [19 20]]\n",
      "centroids_before:\n",
      " [[ 1.  2.]\n",
      " [11. 12.]\n",
      " [19. 20.]]\n",
      "centroids_after:\n",
      " [[ 2.  3.]\n",
      " [12. 13.]\n",
      " [19. 20.]]\n",
      "New iteration\n",
      "[np.float64(2.0000000000000004), np.float64(241.99999999999997), np.float64(648.0)]\n",
      "[np.float64(2.0000000000000004), np.float64(162.0), np.float64(512.0000000000001)]\n",
      "[np.float64(162.0), np.float64(2.0000000000000004), np.float64(128.00000000000003)]\n",
      "[np.float64(241.99999999999997), np.float64(2.0000000000000004), np.float64(71.99999999999999)]\n",
      "[np.float64(577.9999999999999), np.float64(98.0), np.float64(0.0)]\n",
      "y_pred: [0 0 1 1 2]\n",
      "y_pred: [0 0 1 1 2]\n",
      "data:\n",
      " [[ 1  2]\n",
      " [ 3  4]\n",
      " [11 12]\n",
      " [13 14]\n",
      " [19 20]]\n",
      "centroids_before:\n",
      " [[ 2.  3.]\n",
      " [12. 13.]\n",
      " [19. 20.]]\n",
      "centroids_after:\n",
      " [[ 2.  3.]\n",
      " [12. 13.]\n",
      " [19. 20.]]\n",
      "final centroids:\n",
      " [[ 2.  3.]\n",
      " [12. 13.]\n",
      " [19. 20.]]\n",
      "final y_pred: [0 0 1 1 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2], [3, 4], [11, 12], [13, 14], [19, 20]])\n",
    "\n",
    "kmeans = KMeans(algorithm='lloyd', init='kmeans++')\n",
    "kmeans.fit(a, 3, True)\n",
    "accuracy(np.array([0, 0, 1, 1, 2]), kmeans.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial centroids:\n",
      " [[13 14]\n",
      " [ 3  4]\n",
      " [19 20]]\n",
      "\n",
      "Running Extended Hartigan algorithm...\n",
      "0: [np.float64(287.99999999999994), np.float64(8.000000000000002), np.float64(648.0)]\n",
      "1: [np.float64(200.00000000000003), np.float64(0.0), np.float64(512.0000000000001)]\n",
      "2: [np.float64(8.000000000000002), np.float64(128.00000000000003), np.float64(128.00000000000003)]\n",
      "3: [np.float64(0.0), np.float64(200.00000000000003), np.float64(71.99999999999999)]\n",
      "4: [np.float64(71.99999999999999), np.float64(512.0000000000001), np.float64(0.0)]\n",
      "\n",
      "datapoint_id: 0\n",
      "current_cost: 16.000000000000004\n",
      "delta_cost for datapoint 0 from centroid 1 to centroid 0: 175.99999999999994\n",
      "delta_cost for datapoint 0 from centroid 1 to centroid 2: 308.0\n",
      "\n",
      "datapoint_id: 1\n",
      "current_cost: 0.0\n",
      "delta_cost for datapoint 1 from centroid 1 to centroid 0: 133.33333333333334\n",
      "delta_cost for datapoint 1 from centroid 1 to centroid 2: 256.00000000000006\n",
      "\n",
      "datapoint_id: 2\n",
      "current_cost: 16.000000000000004\n",
      "delta_cost for datapoint 2 from centroid 0 to centroid 1: 69.33333333333334\n",
      "delta_cost for datapoint 2 from centroid 0 to centroid 2: 48.000000000000014\n",
      "\n",
      "datapoint_id: 3\n",
      "current_cost: 0.0\n",
      "delta_cost for datapoint 3 from centroid 0 to centroid 1: 133.33333333333334\n",
      "delta_cost for datapoint 3 from centroid 0 to centroid 2: 35.99999999999999\n",
      "\n",
      "datapoint_id: 4\n",
      "current_cost: 0.0\n",
      "delta_cost for datapoint 4 from centroid 2 to centroid 0: 47.999999999999986\n",
      "delta_cost for datapoint 4 from centroid 2 to centroid 1: 341.33333333333337\n",
      "\n",
      "candidates: {}\n",
      "no more candidates\n",
      "final centroids:\n",
      " [[13 14]\n",
      " [ 3  4]\n",
      " [19 20]]\n",
      "final y_pred: [1 1 0 0 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2], [3, 4], [11, 12], [13, 14], [19, 20]])\n",
    "\n",
    "kmeans = KMeans(algorithm='extended-hartigan', init='random')\n",
    "kmeans.fit(a, 3, True)\n",
    "accuracy(np.array([0, 0, 1, 1, 2]), kmeans.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial centroids:\n",
      " [[19 20]\n",
      " [11 12]\n",
      " [ 1  2]]\n",
      "\n",
      "Running Extended Hartigan algorithm...\n",
      "0: [np.float64(648.0), np.float64(200.00000000000003), np.float64(0.0)]\n",
      "1: [np.float64(512.0000000000001), np.float64(128.00000000000003), np.float64(8.000000000000002)]\n",
      "2: [np.float64(128.00000000000003), np.float64(0.0), np.float64(200.00000000000003)]\n",
      "3: [np.float64(71.99999999999999), np.float64(8.000000000000002), np.float64(287.99999999999994)]\n",
      "4: [np.float64(0.0), np.float64(128.00000000000003), np.float64(648.0)]\n",
      "\n",
      "datapoint_id: 0\n",
      "current_cost: 0.0\n",
      "delta_cost for datapoint 0 from centroid 2 to centroid 0: 324.0\n",
      "delta_cost for datapoint 0 from centroid 2 to centroid 1: 133.33333333333334\n",
      "\n",
      "datapoint_id: 1\n",
      "current_cost: 16.000000000000004\n",
      "delta_cost for datapoint 1 from centroid 2 to centroid 0: 240.00000000000006\n",
      "delta_cost for datapoint 1 from centroid 2 to centroid 1: 69.33333333333334\n",
      "\n",
      "datapoint_id: 2\n",
      "current_cost: 0.0\n",
      "delta_cost for datapoint 2 from centroid 1 to centroid 0: 64.00000000000001\n",
      "delta_cost for datapoint 2 from centroid 1 to centroid 2: 133.33333333333334\n",
      "\n",
      "datapoint_id: 3\n",
      "current_cost: 16.000000000000004\n",
      "delta_cost for datapoint 3 from centroid 1 to centroid 0: 19.99999999999999\n",
      "delta_cost for datapoint 3 from centroid 1 to centroid 2: 175.99999999999994\n",
      "\n",
      "datapoint_id: 4\n",
      "current_cost: 0.0\n",
      "delta_cost for datapoint 4 from centroid 0 to centroid 1: 85.33333333333334\n",
      "delta_cost for datapoint 4 from centroid 0 to centroid 2: 432.0\n",
      "\n",
      "candidates: {}\n",
      "no more candidates\n",
      "final centroids:\n",
      " [[19 20]\n",
      " [11 12]\n",
      " [ 1  2]]\n",
      "final y_pred: [2 2 1 1 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2], [3, 4], [11, 12], [13, 14], [19, 20]])\n",
    "\n",
    "kmeans = KMeans(algorithm='safe-hartigan', init='random')\n",
    "kmeans.fit(a, 3, True)\n",
    "accuracy(np.array([0, 0, 1, 1, 2]), kmeans.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.37MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 278kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.50MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mnist = datasets.MNIST('data', train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[442], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m mnist\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m)\n\u001b[0;32m      3\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextended-hartigan\u001b[39m\u001b[38;5;124m'\u001b[39m, init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[437], line 78\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, data, k, debug)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lloyd(debug)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextended-hartigan\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extended_hartigan\u001b[49m\u001b[43m(\u001b[49m\u001b[43malways_safe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msafe-hartigan\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_hartigan(always_safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, debug\u001b[38;5;241m=\u001b[39mdebug)\n",
      "Cell \u001b[1;32mIn[437], line 207\u001b[0m, in \u001b[0;36mKMeans._extended_hartigan\u001b[1;34m(self, always_safe, debug)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# iterate only on possible new centroid assignments\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m centroid_id \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39msetdiff1d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_pred, current_centroid_id):\n\u001b[1;32m--> 207\u001b[0m     delta_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_delta_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_cost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatapoint_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroid_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m     debug \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta_cost for datapoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatapoint_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from centroid \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_centroid_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to centroid \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcentroid_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m, delta_cost)\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# datapoint is a candidate if it reduces the cost\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# if more reassignments reduce the cost, the best one is stored (the one producing the most negfative delta_cost)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[437], line 318\u001b[0m, in \u001b[0;36mKMeans._delta_cost\u001b[1;34m(self, cost, datapoint_id, centroid_id)\u001b[0m\n\u001b[0;32m    315\u001b[0m prefactor \u001b[38;5;241m=\u001b[39m cluster_size \u001b[38;5;241m/\u001b[39m (cluster_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# cost of new assignment\u001b[39;00m\n\u001b[1;32m--> 318\u001b[0m new_cost \u001b[38;5;241m=\u001b[39m prefactor \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdatapoint_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcentroids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcentroid_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_cost \u001b[38;5;241m-\u001b[39m cost\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\numpy\\linalg\\_linalg.py:2736\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2734\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m x_real\u001b[38;5;241m.\u001b[39mdot(x_real) \u001b[38;5;241m+\u001b[39m x_imag\u001b[38;5;241m.\u001b[39mdot(x_imag)\n\u001b[0;32m   2735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2736\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2737\u001b[0m ret \u001b[38;5;241m=\u001b[39m sqrt(sqnorm)\n\u001b[0;32m   2738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = mnist.data.numpy().reshape(-1, 28*28)\n",
    "kmeans = KMeans(algorithm='extended-hartigan', init='random')\n",
    "kmeans.fit(data[:20], 10)\n",
    "accuracy(mnist.targets.numpy()[:5000], kmeans.y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
